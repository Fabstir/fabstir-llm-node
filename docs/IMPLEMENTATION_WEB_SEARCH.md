# IMPLEMENTATION - Host-Side Web Search

## Status: IN PROGRESS ğŸ”§

**Status**: Phases 1-5, 7-9 Complete | **Phase 9: Content Fetching Complete** âœ…
**Version**: v8.8.0-content-fetch
**Start Date**: 2025-01-05
**Last Updated**: 2026-01-06
**Approach**: Strict TDD bounded autonomy - one sub-phase at a time
**Tests Passing**: 105 search-related tests (all passing)

### Current Issue (Phase 8)
Web search is only implemented in the **non-streaming HTTP inference path**. The SDK uses WebSocket encrypted streaming which bypasses web search entirely. Phase 8 fixes this gap.

---

## Overview

Implementation plan for host-side web search in Fabstir LLM Node. This enables hosts to perform web searches on behalf of clients, supporting use cases from simple lookups to deep research with hundreds of queries - without consuming client bandwidth or mobile data.

**Key Benefits for Decentralized P2P:**
- Hosts have datacenter-quality connections (fast, reliable, unlimited)
- Mobile/low-bandwidth clients send single request, receive synthesized response
- Deep research (100+ searches) impractical on client side
- Search cost built into inference pricing

**New Capabilities:**
- `POST /v1/search` - Standalone web search endpoint
- `web_search` flag in chat/inference requests
- `POST /v1/research` - Deep research with agentic loop
- WebSocket messages for search progress streaming

**Approach**: Strict TDD bounded autonomy - one sub-phase at a time

**Key Constraints:**
- **Privacy-respecting providers** - Brave Search, DuckDuckGo preferred
- **Graceful degradation** - If search fails, continue inference without results
- **Rate limiting** - Prevent abuse of search APIs
- **Caching** - Reduce API costs and latency for repeated queries
- **Host capability** - Search is optional; hosts advertise availability

**References:**
- Existing API patterns: `src/api/embed/`, `src/api/ocr/`
- Session management: `src/api/websocket/`
- Model manager pattern: `src/vision/model_manager.rs`

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CLIENT (Mobile/Web)                            â”‚
â”‚                                                                          â”‚
â”‚  User: "What are the latest AI developments in 2025?"                   â”‚
â”‚                              â”‚                                           â”‚
â”‚         { "prompt": "...", "web_search": true }                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ Single request (~1KB)
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         P2P HOST NODE                                    â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Search Module (src/search/)                    â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  SearchService                                                   â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ BraveSearchProvider                                       â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ DuckDuckGoProvider                                        â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ BingSearchProvider                                        â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ SearchCache (TTL-based)                                   â”‚   â”‚
â”‚  â”‚    â””â”€â”€ RateLimiter                                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Research Agent (src/research/) [Phase 4]            â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  1. Generate research plan (LLM)                                 â”‚   â”‚
â”‚  â”‚  2. Execute searches (parallel batches)                          â”‚   â”‚
â”‚  â”‚  3. Synthesize results (LLM)                                     â”‚   â”‚
â”‚  â”‚  4. Identify gaps â†’ more searches                                â”‚   â”‚
â”‚  â”‚  5. Final synthesis                                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    LLM Inference Engine                          â”‚   â”‚
â”‚  â”‚                    (llama-cpp-2 + CUDA)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dependencies Required

### Cargo.toml Updates
```toml
[dependencies]
# Existing dependencies (no changes needed):
# reqwest = { version = "0.11", features = ["json", "rustls-tls"] }
# tokio = { version = "1", features = ["full"] }
# serde = { version = "1", features = ["derive"] }
# serde_json = "1"
# tracing = "0.1"

# Rate limiting (NEW - may already have similar)
governor = "0.6"                              # Token bucket rate limiting

# URL handling (likely already present)
url = "2"                                     # URL parsing
```

**Note**: Most dependencies already present in the project. Governor adds robust rate limiting.

---

## Environment Variables

```bash
# Web Search Configuration
WEB_SEARCH_ENABLED=true                       # Enable/disable search feature
BRAVE_API_KEY=BSA...                          # Brave Search API key (recommended)
BING_API_KEY=...                              # Bing Search API key (optional)
SEARCH_CACHE_TTL_SECS=3600                    # Cache TTL (default: 1 hour)
SEARCH_RATE_LIMIT_PER_MINUTE=60               # Rate limit per minute
MAX_SEARCHES_PER_REQUEST=20                   # Max searches per single request
MAX_SEARCHES_PER_SESSION=200                  # Max searches per session
DEEP_RESEARCH_MAX_ITERATIONS=5                # Max research iterations
DEEP_RESEARCH_MAX_SEARCHES=100                # Max searches for deep research
```

---

## Module Structure

```
src/
â”œâ”€â”€ search/
â”‚   â”œâ”€â”€ mod.rs                 # Public exports
â”‚   â”œâ”€â”€ types.rs               # SearchResult, SearchError, SearchResponse
â”‚   â”œâ”€â”€ config.rs              # SearchConfig, environment loading
â”‚   â”œâ”€â”€ provider.rs            # SearchProvider trait
â”‚   â”œâ”€â”€ brave.rs               # Brave Search implementation
â”‚   â”œâ”€â”€ duckduckgo.rs          # DuckDuckGo implementation (no API key)
â”‚   â”œâ”€â”€ bing.rs                # Bing Search implementation
â”‚   â”œâ”€â”€ cache.rs               # TTL-based result caching
â”‚   â”œâ”€â”€ rate_limiter.rs        # Rate limiting per provider
â”‚   â”œâ”€â”€ service.rs             # SearchService orchestration
â”‚   â””â”€â”€ query_extractor.rs     # Extract search queries from prompts
â”‚
â”œâ”€â”€ research/                  # Phase 4
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ agent.rs               # Agentic research loop
â”‚   â”œâ”€â”€ planner.rs             # Generate research plans via LLM
â”‚   â””â”€â”€ synthesizer.rs         # Combine and summarize results
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ handler.rs         # /v1/search endpoint
â”‚   â”‚   â”œâ”€â”€ request.rs         # SearchRequest validation
â”‚   â”‚   â””â”€â”€ response.rs        # SearchApiResponse
â”‚   â””â”€â”€ research/              # Phase 4
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ handler.rs         # /v1/research endpoint
â”‚       â”œâ”€â”€ request.rs
â”‚       â””â”€â”€ response.rs
```

---

## Phase 1: Foundation (2 hours) âœ… COMPLETE

### Sub-phase 1.1: Add Dependencies âœ…

**Goal**: Add required crates and verify build

**Status**: COMPLETE (2025-01-05)

#### Tasks
- [x] Add `governor = "0.6"` to Cargo.toml (if not present)
- [x] Add `url = "2"` to Cargo.toml for URL parsing
- [x] Verify `reqwest`, `serde` dependencies present
- [x] Run `cargo check` to verify dependencies compile
- [x] Run existing tests to ensure no regressions (445 passed, 4 pre-existing failures)

**Test Files:**
- Run `cargo test --lib` - Ensure no regressions

**Implementation Files:**
- `Cargo.toml` - Added dependencies:
  - `governor = "0.6"` (line 89) - Rate limiting for search API requests
  - `url = "2"` (line 85) - URL parsing and validation

---

### Sub-phase 1.2: Create Module Structure âœ…

**Goal**: Create stub files for all new modules

**Status**: COMPLETE (2025-01-05)

#### Tasks
- [x] Create `src/search/mod.rs` with submodule declarations
- [x] Create `src/search/types.rs` with SearchResult, SearchResponse, SearchError
- [x] Create `src/search/config.rs` with SearchConfig, from_env()
- [x] Create `src/search/provider.rs` with SearchProvider trait
- [x] Create `src/search/brave.rs` with BraveSearchProvider
- [x] Create `src/search/duckduckgo.rs` with DuckDuckGoProvider
- [x] Create `src/search/bing.rs` with BingSearchProvider
- [x] Create `src/search/cache.rs` with SearchCache (TTL-based)
- [x] Create `src/search/rate_limiter.rs` with SearchRateLimiter (governor)
- [x] Create `src/search/service.rs` with SearchService orchestration
- [x] Create `src/search/query_extractor.rs` with query extraction utilities
- [x] Create `src/api/search/mod.rs` with re-exports
- [x] Create `src/api/search/handler.rs` with search_handler
- [x] Create `src/api/search/request.rs` with SearchApiRequest
- [x] Create `src/api/search/response.rs` with SearchApiResponse
- [x] Add `pub mod search;` to `src/lib.rs`
- [x] Add `pub mod search;` to `src/api/mod.rs`
- [x] Add `search_service` to AppState in `src/api/http_server.rs`
- [x] Add `/v1/search` route to create_app()
- [x] Run `cargo check` to verify module structure (298 warnings, 0 errors)
- [x] Run `cargo test search::` - 74 tests passed

**Files Created:**
- `src/search/mod.rs` - Module declarations and re-exports
- `src/search/types.rs` - Core types (SearchResult, SearchResponse, SearchError)
- `src/search/config.rs` - Configuration with env var loading
- `src/search/provider.rs` - SearchProvider async trait
- `src/search/brave.rs` - Brave Search API implementation
- `src/search/duckduckgo.rs` - DuckDuckGo HTML parser (no API key)
- `src/search/bing.rs` - Bing Search API implementation
- `src/search/cache.rs` - TTL-based result caching
- `src/search/rate_limiter.rs` - Governor-based rate limiting
- `src/search/service.rs` - SearchService orchestration
- `src/search/query_extractor.rs` - Query extraction and detection
- `src/api/search/mod.rs` - API module re-exports
- `src/api/search/handler.rs` - POST /v1/search handler
- `src/api/search/request.rs` - Request types with validation
- `src/api/search/response.rs` - Response types with chain context

**Files Modified:**
- `src/lib.rs` - Added `pub mod search;`
- `src/api/mod.rs` - Added `pub mod search;` and re-exports
- `src/api/http_server.rs` - Added `search_service` to AppState, `/v1/search` route
- `src/api/server.rs` - Added `search_service` to wrapper AppState instances

---

### Sub-phase 1.3: Define Core Types âœ…

**Goal**: Define search types with serialization

**Status**: COMPLETE (2025-01-05) - Implemented in Sub-phase 1.2

#### Tasks
- [x] Write tests for SearchResult serialization/deserialization (5 tests)
- [x] Write tests for SearchResponse serialization (3 tests)
- [x] Write tests for SearchError variants (4 tests)
- [x] Implement `SearchResult` struct
- [x] Implement `SearchResponse` struct
- [x] Implement `SearchError` enum with thiserror
- [x] Implement `SearchQuery` struct for batch operations

**Test Files:**
- Inline tests in `src/search/types.rs` (5 tests)

**Implementation Files:**
- `src/search/types.rs` (max 200 lines)
  ```rust
  use serde::{Deserialize, Serialize};
  use thiserror::Error;

  #[derive(Debug, Clone, Serialize, Deserialize)]
  #[serde(rename_all = "camelCase")]
  pub struct SearchResult {
      pub title: String,
      pub url: String,
      pub snippet: String,
      #[serde(skip_serializing_if = "Option::is_none")]
      pub published_date: Option<String>,
      pub source: String,  // "brave", "bing", "duckduckgo"
  }

  #[derive(Debug, Clone, Serialize, Deserialize)]
  #[serde(rename_all = "camelCase")]
  pub struct SearchResponse {
      pub query: String,
      pub results: Vec<SearchResult>,
      pub search_time_ms: u64,
      pub provider: String,
      pub cached: bool,
      pub result_count: usize,
  }

  #[derive(Debug, Error)]
  pub enum SearchError {
      #[error("Rate limited, retry after {retry_after_secs}s")]
      RateLimited { retry_after_secs: u64 },

      #[error("Search API error: {status} - {message}")]
      ApiError { status: u16, message: String },

      #[error("Search timeout after {timeout_ms}ms")]
      Timeout { timeout_ms: u64 },

      #[error("Provider unavailable: {provider}")]
      ProviderUnavailable { provider: String },

      #[error("No API key configured for {provider}")]
      NoApiKey { provider: String },

      #[error("Invalid query: {reason}")]
      InvalidQuery { reason: String },

      #[error("Search disabled on this host")]
      SearchDisabled,
  }

  #[derive(Debug, Clone)]
  pub struct SearchQuery {
      pub query: String,
      pub num_results: usize,
      pub request_id: Option<String>,
  }
  ```

---

### Sub-phase 1.4: Define Configuration âœ…

**Goal**: Define configuration with environment variable loading

**Status**: COMPLETE (2025-01-05) - Implemented in Sub-phase 1.2

#### Tasks
- [x] Write tests for SearchConfig defaults (3 tests)
- [x] Write tests for environment variable loading (5 tests)
- [x] Write tests for validation (3 tests)
- [x] Implement `SearchConfig` struct
- [x] Implement `SearchProviderConfig` struct
- [x] Implement `from_env()` loading function
- [x] Add validation for config values

**Test Files:**
- Inline tests in `src/search/config.rs` (7 tests)

**Implementation Files:**
- `src/search/config.rs` (max 200 lines)
  ```rust
  use std::env;

  #[derive(Debug, Clone)]
  pub struct SearchConfig {
      pub enabled: bool,
      pub providers: SearchProviderConfig,
      pub cache_ttl_secs: u64,
      pub max_searches_per_request: u32,
      pub max_searches_per_session: u32,
      pub rate_limit_per_minute: u32,
      pub default_num_results: usize,
      pub request_timeout_ms: u64,
  }

  #[derive(Debug, Clone)]
  pub struct SearchProviderConfig {
      pub brave_api_key: Option<String>,
      pub bing_api_key: Option<String>,
      pub preferred_provider: String,
  }

  impl SearchConfig {
      pub fn from_env() -> Self {
          Self {
              enabled: env::var("WEB_SEARCH_ENABLED")
                  .map(|v| v.to_lowercase() == "true")
                  .unwrap_or(false),
              providers: SearchProviderConfig {
                  brave_api_key: env::var("BRAVE_API_KEY").ok(),
                  bing_api_key: env::var("BING_API_KEY").ok(),
                  preferred_provider: env::var("SEARCH_PROVIDER")
                      .unwrap_or_else(|_| "brave".to_string()),
              },
              cache_ttl_secs: env::var("SEARCH_CACHE_TTL_SECS")
                  .ok()
                  .and_then(|v| v.parse().ok())
                  .unwrap_or(3600),
              max_searches_per_request: env::var("MAX_SEARCHES_PER_REQUEST")
                  .ok()
                  .and_then(|v| v.parse().ok())
                  .unwrap_or(20),
              max_searches_per_session: env::var("MAX_SEARCHES_PER_SESSION")
                  .ok()
                  .and_then(|v| v.parse().ok())
                  .unwrap_or(200),
              rate_limit_per_minute: env::var("SEARCH_RATE_LIMIT_PER_MINUTE")
                  .ok()
                  .and_then(|v| v.parse().ok())
                  .unwrap_or(60),
              default_num_results: 10,
              request_timeout_ms: 10000,
          }
      }

      pub fn validate(&self) -> Result<(), String> {
          if self.enabled && self.providers.brave_api_key.is_none()
              && self.providers.bing_api_key.is_none() {
              return Err("Search enabled but no API keys configured".to_string());
          }
          Ok(())
      }

      pub fn has_any_provider(&self) -> bool {
          self.providers.brave_api_key.is_some()
              || self.providers.bing_api_key.is_some()
      }
  }

  impl Default for SearchConfig {
      fn default() -> Self {
          Self {
              enabled: false,
              providers: SearchProviderConfig {
                  brave_api_key: None,
                  bing_api_key: None,
                  preferred_provider: "brave".to_string(),
              },
              cache_ttl_secs: 3600,
              max_searches_per_request: 20,
              max_searches_per_session: 200,
              rate_limit_per_minute: 60,
              default_num_results: 10,
              request_timeout_ms: 10000,
          }
      }
  }
  ```

---

## Phase 2: Search Providers (4 hours) âœ… COMPLETE

### Sub-phase 2.1: SearchProvider Trait âœ…

**Goal**: Define trait for search providers with async support

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for trait object usage (2 tests)
- [x] Define `SearchProvider` async trait
- [x] Add provider metadata methods
- [x] Add availability check method

**Test Files:**
- Inline tests in `src/search/provider.rs` (3 tests)

**Implementation Files:**
- `src/search/provider.rs` (max 100 lines)
  ```rust
  use async_trait::async_trait;
  use super::types::{SearchResult, SearchError};

  #[async_trait]
  pub trait SearchProvider: Send + Sync {
      /// Perform a web search
      async fn search(
          &self,
          query: &str,
          num_results: usize,
      ) -> Result<Vec<SearchResult>, SearchError>;

      /// Provider name for logging and billing
      fn name(&self) -> &'static str;

      /// Check if provider is available (has API key, etc.)
      fn is_available(&self) -> bool;

      /// Provider priority (lower = preferred)
      fn priority(&self) -> u8 {
          100
      }
  }
  ```

---

### Sub-phase 2.2: Brave Search Provider âœ…

**Goal**: Implement Brave Search API integration

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for BraveSearchProvider creation (3 tests)
- [x] Write tests for successful search response parsing (4 tests)
- [x] Write tests for error handling (rate limit, auth, timeout) (5 tests)
- [x] Write tests for result transformation (3 tests)
- [x] Implement `BraveSearchProvider` struct
- [x] Implement API request construction
- [x] Implement response parsing
- [x] Add proper error handling for all status codes

**Test Files:**
- Inline tests in `src/search/brave.rs` (2 tests)

**Implementation Files:**
- `src/search/brave.rs` (max 250 lines)
  ```rust
  use reqwest::Client;
  use std::time::Duration;
  use async_trait::async_trait;
  use super::provider::SearchProvider;
  use super::types::{SearchResult, SearchError};

  const BRAVE_API_URL: &str = "https://api.search.brave.com/res/v1/web/search";

  pub struct BraveSearchProvider {
      api_key: String,
      client: Client,
  }

  impl BraveSearchProvider {
      pub fn new(api_key: String) -> Self {
          let client = Client::builder()
              .timeout(Duration::from_secs(10))
              .build()
              .expect("Failed to create HTTP client");

          Self { api_key, client }
      }
  }

  #[async_trait]
  impl SearchProvider for BraveSearchProvider {
      async fn search(
          &self,
          query: &str,
          num_results: usize,
      ) -> Result<Vec<SearchResult>, SearchError> {
          let response = self.client
              .get(BRAVE_API_URL)
              .header("X-Subscription-Token", &self.api_key)
              .header("Accept", "application/json")
              .query(&[
                  ("q", query),
                  ("count", &num_results.min(20).to_string()),
              ])
              .send()
              .await
              .map_err(|e| {
                  if e.is_timeout() {
                      SearchError::Timeout { timeout_ms: 10000 }
                  } else {
                      SearchError::ApiError {
                          status: 0,
                          message: e.to_string(),
                      }
                  }
              })?;

          let status = response.status();

          if status == 429 {
              return Err(SearchError::RateLimited { retry_after_secs: 60 });
          }

          if status == 401 || status == 403 {
              return Err(SearchError::NoApiKey {
                  provider: "brave".to_string()
              });
          }

          if !status.is_success() {
              let message = response.text().await.unwrap_or_default();
              return Err(SearchError::ApiError {
                  status: status.as_u16(),
                  message,
              });
          }

          let data: BraveResponse = response.json().await.map_err(|e| {
              SearchError::ApiError {
                  status: 0,
                  message: format!("JSON parse error: {}", e),
              }
          })?;

          Ok(data.web.results.into_iter().map(|r| SearchResult {
              title: r.title,
              url: r.url,
              snippet: r.description,
              published_date: r.age,
              source: "brave".to_string(),
          }).collect())
      }

      fn name(&self) -> &'static str {
          "brave"
      }

      fn is_available(&self) -> bool {
          !self.api_key.is_empty()
      }

      fn priority(&self) -> u8 {
          10  // Preferred provider
      }
  }

  #[derive(Debug, serde::Deserialize)]
  struct BraveResponse {
      web: BraveWebResults,
  }

  #[derive(Debug, serde::Deserialize)]
  struct BraveWebResults {
      results: Vec<BraveResult>,
  }

  #[derive(Debug, serde::Deserialize)]
  struct BraveResult {
      title: String,
      url: String,
      description: String,
      age: Option<String>,
  }
  ```

---

### Sub-phase 2.3: DuckDuckGo Provider âœ…

**Goal**: Implement DuckDuckGo search (no API key required)

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for DuckDuckGoProvider creation (2 tests)
- [x] Write tests for HTML parsing (4 tests)
- [x] Write tests for rate limiting handling (2 tests)
- [x] Implement `DuckDuckGoProvider` struct
- [x] Implement HTML scraping approach (DDG has no official API)
- [x] Add respectful rate limiting

**Test Files:**
- Inline tests in `src/search/duckduckgo.rs` (7 tests)

**Implementation Files:**
- `src/search/duckduckgo.rs` (max 200 lines)
  ```rust
  use reqwest::Client;
  use std::time::Duration;
  use async_trait::async_trait;
  use super::provider::SearchProvider;
  use super::types::{SearchResult, SearchError};

  const DDG_HTML_URL: &str = "https://html.duckduckgo.com/html/";

  pub struct DuckDuckGoProvider {
      client: Client,
  }

  impl DuckDuckGoProvider {
      pub fn new() -> Self {
          let client = Client::builder()
              .timeout(Duration::from_secs(10))
              .user_agent("Mozilla/5.0 (compatible; FabstirBot/1.0)")
              .build()
              .expect("Failed to create HTTP client");

          Self { client }
      }
  }

  #[async_trait]
  impl SearchProvider for DuckDuckGoProvider {
      async fn search(
          &self,
          query: &str,
          num_results: usize,
      ) -> Result<Vec<SearchResult>, SearchError> {
          let response = self.client
              .post(DDG_HTML_URL)
              .form(&[("q", query)])
              .send()
              .await
              .map_err(|e| SearchError::ApiError {
                  status: 0,
                  message: e.to_string(),
              })?;

          let html = response.text().await.map_err(|e| {
              SearchError::ApiError {
                  status: 0,
                  message: e.to_string(),
              }
          })?;

          // Parse HTML for results (simplified)
          let results = parse_ddg_html(&html, num_results);

          Ok(results)
      }

      fn name(&self) -> &'static str {
          "duckduckgo"
      }

      fn is_available(&self) -> bool {
          true  // No API key needed
      }

      fn priority(&self) -> u8 {
          50  // Fallback provider
      }
  }

  fn parse_ddg_html(html: &str, max_results: usize) -> Vec<SearchResult> {
      // Simple regex-based parsing
      // In production, use scraper crate for robust HTML parsing
      let mut results = Vec::new();

      // Parse result divs - this is simplified
      // Real implementation would use proper HTML parsing

      results.truncate(max_results);
      results
  }
  ```

---

### Sub-phase 2.4: Bing Search Provider (Optional) âœ…

**Goal**: Implement Bing Search API as alternative

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for BingSearchProvider creation (3 tests)
- [x] Write tests for response parsing (3 tests)
- [x] Write tests for error handling (3 tests)
- [x] Implement `BingSearchProvider` struct
- [x] Implement Bing Web Search API v7

**Test Files:**
- Inline tests in `src/search/bing.rs` (2 tests)

**Implementation Files:**
- `src/search/bing.rs` (max 200 lines)
  ```rust
  // Similar structure to Brave provider
  // Bing API: https://api.bing.microsoft.com/v7.0/search
  ```

---

## Phase 3: Search Service (3 hours) âœ… COMPLETE

### Sub-phase 3.1: Search Cache âœ…

**Goal**: Implement TTL-based caching for search results

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for cache insertion and retrieval (4 tests)
- [x] Write tests for TTL expiration (3 tests)
- [x] Write tests for cache key generation (2 tests)
- [x] Write tests for cache size limits (2 tests)
- [x] Implement `SearchCache` struct with HashMap
- [x] Implement TTL-based expiration
- [x] Add cache statistics method
- [x] Add cache clear method

**Test Files:**
- Inline tests in `src/search/cache.rs` (13 tests)

**Implementation Files:**
- `src/search/cache.rs` (max 200 lines)
  ```rust
  use std::collections::HashMap;
  use std::sync::RwLock;
  use std::time::{Duration, Instant};
  use super::types::SearchResult;

  pub struct SearchCache {
      cache: RwLock<HashMap<String, CachedEntry>>,
      ttl: Duration,
      max_entries: usize,
  }

  struct CachedEntry {
      results: Vec<SearchResult>,
      provider: String,
      inserted_at: Instant,
  }

  impl SearchCache {
      pub fn new(ttl_secs: u64, max_entries: usize) -> Self {
          Self {
              cache: RwLock::new(HashMap::new()),
              ttl: Duration::from_secs(ttl_secs),
              max_entries,
          }
      }

      pub fn get(&self, query: &str) -> Option<(Vec<SearchResult>, String)> {
          let cache = self.cache.read().ok()?;
          let key = Self::cache_key(query);
          let entry = cache.get(&key)?;

          if entry.inserted_at.elapsed() > self.ttl {
              return None;  // Expired
          }

          Some((entry.results.clone(), entry.provider.clone()))
      }

      pub fn insert(&self, query: &str, results: &[SearchResult], provider: &str) {
          let mut cache = match self.cache.write() {
              Ok(c) => c,
              Err(_) => return,
          };

          // Evict oldest if at capacity
          if cache.len() >= self.max_entries {
              self.evict_oldest(&mut cache);
          }

          let key = Self::cache_key(query);
          cache.insert(key, CachedEntry {
              results: results.to_vec(),
              provider: provider.to_string(),
              inserted_at: Instant::now(),
          });
      }

      pub fn clear(&self) {
          if let Ok(mut cache) = self.cache.write() {
              cache.clear();
          }
      }

      pub fn stats(&self) -> CacheStats {
          let cache = self.cache.read().unwrap();
          let total = cache.len();
          let expired = cache.values()
              .filter(|e| e.inserted_at.elapsed() > self.ttl)
              .count();
          CacheStats { total, expired, max: self.max_entries }
      }

      fn cache_key(query: &str) -> String {
          query.to_lowercase().trim().to_string()
      }

      fn evict_oldest(&self, cache: &mut HashMap<String, CachedEntry>) {
          if let Some(oldest_key) = cache.iter()
              .min_by_key(|(_, v)| v.inserted_at)
              .map(|(k, _)| k.clone())
          {
              cache.remove(&oldest_key);
          }
      }
  }

  #[derive(Debug)]
  pub struct CacheStats {
      pub total: usize,
      pub expired: usize,
      pub max: usize,
  }
  ```

---

### Sub-phase 3.2: Rate Limiter âœ…

**Goal**: Implement rate limiting for search requests

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for rate limiter allowing requests (3 tests)
- [x] Write tests for rate limiter blocking excess (3 tests)
- [x] Write tests for rate limiter reset (2 tests)
- [x] Implement `SearchRateLimiter` using governor
- [x] Add per-provider rate limiting
- [x] Add global rate limiting

**Test Files:**
- Inline tests in `src/search/rate_limiter.rs` (5 tests)

**Implementation Files:**
- `src/search/rate_limiter.rs` (max 150 lines)
  ```rust
  use governor::{Quota, RateLimiter as GovRateLimiter};
  use governor::clock::DefaultClock;
  use governor::state::{InMemoryState, NotKeyed};
  use std::num::NonZeroU32;
  use std::sync::Arc;
  use super::types::SearchError;

  pub struct SearchRateLimiter {
      limiter: Arc<GovRateLimiter<NotKeyed, InMemoryState, DefaultClock>>,
      requests_per_minute: u32,
  }

  impl SearchRateLimiter {
      pub fn new(requests_per_minute: u32) -> Self {
          let quota = Quota::per_minute(
              NonZeroU32::new(requests_per_minute).unwrap_or(NonZeroU32::new(60).unwrap())
          );
          let limiter = Arc::new(GovRateLimiter::direct(quota));

          Self {
              limiter,
              requests_per_minute,
          }
      }

      pub fn check(&self) -> Result<(), SearchError> {
          match self.limiter.check() {
              Ok(_) => Ok(()),
              Err(_) => Err(SearchError::RateLimited {
                  retry_after_secs: 60
              }),
          }
      }

      pub async fn wait(&self) {
          self.limiter.until_ready().await;
      }

      pub fn requests_per_minute(&self) -> u32 {
          self.requests_per_minute
      }
  }
  ```

---

### Sub-phase 3.3: Search Service âœ…

**Goal**: Orchestrate providers, cache, and rate limiting

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for SearchService creation with config (3 tests)
- [x] Write tests for single search with cache miss (3 tests)
- [x] Write tests for single search with cache hit (2 tests)
- [x] Write tests for provider failover (3 tests)
- [x] Write tests for batch search (4 tests)
- [x] Write tests for rate limiting integration (2 tests)
- [x] Implement `SearchService` struct
- [x] Implement single `search()` method
- [x] Implement `batch_search()` for multiple queries
- [x] Add provider selection and failover logic

**Test Files:**
- Inline tests in `src/search/service.rs` (7 tests)

**Implementation Files:**
- `src/search/service.rs` (max 350 lines)
  ```rust
  use std::sync::Arc;
  use std::time::Instant;
  use tracing::{debug, warn, info};
  use super::config::SearchConfig;
  use super::provider::SearchProvider;
  use super::brave::BraveSearchProvider;
  use super::duckduckgo::DuckDuckGoProvider;
  use super::cache::SearchCache;
  use super::rate_limiter::SearchRateLimiter;
  use super::types::{SearchResult, SearchResponse, SearchError};

  pub struct SearchService {
      providers: Vec<Box<dyn SearchProvider>>,
      cache: SearchCache,
      rate_limiter: SearchRateLimiter,
      config: SearchConfig,
  }

  impl SearchService {
      pub fn new(config: SearchConfig) -> Self {
          let mut providers: Vec<Box<dyn SearchProvider>> = Vec::new();

          // Add Brave if configured
          if let Some(ref api_key) = config.providers.brave_api_key {
              providers.push(Box::new(BraveSearchProvider::new(api_key.clone())));
          }

          // Always add DuckDuckGo as fallback
          providers.push(Box::new(DuckDuckGoProvider::new()));

          // Sort by priority
          providers.sort_by_key(|p| p.priority());

          let cache = SearchCache::new(config.cache_ttl_secs, 1000);
          let rate_limiter = SearchRateLimiter::new(config.rate_limit_per_minute);

          Self {
              providers,
              cache,
              rate_limiter,
              config,
          }
      }

      pub async fn search(
          &self,
          query: &str,
          num_results: Option<usize>,
      ) -> Result<SearchResponse, SearchError> {
          if !self.config.enabled {
              return Err(SearchError::SearchDisabled);
          }

          let num_results = num_results.unwrap_or(self.config.default_num_results);

          // Check cache first
          if let Some((results, provider)) = self.cache.get(query) {
              debug!("Cache hit for query: {}", query);
              return Ok(SearchResponse {
                  query: query.to_string(),
                  results: results.clone(),
                  search_time_ms: 0,
                  provider,
                  cached: true,
                  result_count: results.len(),
              });
          }

          // Rate limit check
          self.rate_limiter.check()?;

          let start = Instant::now();

          // Try providers in order
          for provider in &self.providers {
              if !provider.is_available() {
                  continue;
              }

              debug!("Trying search provider: {}", provider.name());

              match provider.search(query, num_results).await {
                  Ok(results) => {
                      let elapsed_ms = start.elapsed().as_millis() as u64;

                      // Cache successful results
                      self.cache.insert(query, &results, provider.name());

                      info!(
                          "Search complete: {} results from {} in {}ms",
                          results.len(),
                          provider.name(),
                          elapsed_ms
                      );

                      return Ok(SearchResponse {
                          query: query.to_string(),
                          results: results.clone(),
                          search_time_ms: elapsed_ms,
                          provider: provider.name().to_string(),
                          cached: false,
                          result_count: results.len(),
                      });
                  }
                  Err(e) => {
                      warn!(
                          "Search provider {} failed: {}, trying next",
                          provider.name(),
                          e
                      );
                      continue;
                  }
              }
          }

          Err(SearchError::ProviderUnavailable {
              provider: "all".to_string(),
          })
      }

      pub async fn batch_search(
          &self,
          queries: Vec<String>,
          num_results_per_query: Option<usize>,
      ) -> Vec<Result<SearchResponse, SearchError>> {
          let futures: Vec<_> = queries
              .into_iter()
              .map(|q| self.search(&q, num_results_per_query))
              .collect();

          futures::future::join_all(futures).await
      }

      pub fn is_enabled(&self) -> bool {
          self.config.enabled
      }

      pub fn available_providers(&self) -> Vec<&str> {
          self.providers
              .iter()
              .filter(|p| p.is_available())
              .map(|p| p.name())
              .collect()
      }

      pub fn cache_stats(&self) -> super::cache::CacheStats {
          self.cache.stats()
      }
  }
  ```

---

## Phase 4: API Integration (3 hours) âœ… COMPLETE

### Sub-phase 4.1: Request/Response Types âœ…

**Goal**: Define API request and response types

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for SearchApiRequest serialization (4 tests)
- [x] Write tests for SearchApiRequest validation (5 tests)
- [x] Write tests for SearchApiResponse serialization (3 tests)
- [x] Implement `SearchApiRequest` struct with validation
- [x] Implement `SearchApiResponse` struct

**Test Files:**
- Inline tests in `src/api/search/request.rs` (7 tests)
- Inline tests in `src/api/search/response.rs` (4 tests)

**Implementation Files:**
- `src/api/search/request.rs` (max 150 lines)
  ```rust
  use serde::{Deserialize, Serialize};

  #[derive(Debug, Clone, Serialize, Deserialize)]
  #[serde(rename_all = "camelCase")]
  pub struct SearchApiRequest {
      /// Search query string
      pub query: String,

      /// Number of results to return (1-20, default 10)
      #[serde(default = "default_num_results")]
      pub num_results: usize,

      /// Chain ID for billing context
      #[serde(default = "default_chain_id")]
      pub chain_id: u64,

      /// Optional request ID for tracking
      #[serde(skip_serializing_if = "Option::is_none")]
      pub request_id: Option<String>,
  }

  fn default_num_results() -> usize { 10 }
  fn default_chain_id() -> u64 { 84532 }

  impl SearchApiRequest {
      pub fn validate(&self) -> Result<(), String> {
          if self.query.trim().is_empty() {
              return Err("Query cannot be empty".to_string());
          }
          if self.query.len() > 500 {
              return Err("Query too long (max 500 chars)".to_string());
          }
          if self.num_results < 1 || self.num_results > 20 {
              return Err("num_results must be between 1 and 20".to_string());
          }
          Ok(())
      }
  }
  ```

- `src/api/search/response.rs` (max 100 lines)
  ```rust
  use serde::{Deserialize, Serialize};
  use crate::search::types::SearchResult;

  #[derive(Debug, Clone, Serialize, Deserialize)]
  #[serde(rename_all = "camelCase")]
  pub struct SearchApiResponse {
      pub query: String,
      pub results: Vec<SearchResult>,
      pub result_count: usize,
      pub search_time_ms: u64,
      pub provider: String,
      pub cached: bool,
      pub chain_id: u64,
      pub chain_name: String,
  }

  impl SearchApiResponse {
      pub fn new(
          query: String,
          results: Vec<SearchResult>,
          search_time_ms: u64,
          provider: String,
          cached: bool,
          chain_id: u64,
      ) -> Self {
          let chain_name = match chain_id {
              84532 => "Base Sepolia".to_string(),
              5611 => "opBNB Testnet".to_string(),
              _ => "Unknown".to_string(),
          };

          Self {
              result_count: results.len(),
              query,
              results,
              search_time_ms,
              provider,
              cached,
              chain_id,
              chain_name,
          }
      }
  }
  ```

---

### Sub-phase 4.2: Search Handler âœ…

**Goal**: Implement POST /v1/search HTTP handler

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Write tests for successful search (3 tests)
- [x] Write tests for validation errors (4 tests)
- [x] Write tests for search disabled (2 tests)
- [x] Write tests for provider errors (2 tests)
- [x] Implement `search_handler` function
- [x] Add AppState integration for SearchService
- [x] Add proper error response codes

**Test Files:**
- Inline tests in `src/api/search/handler.rs` (unit tests)

**Implementation Files:**
- `src/api/search/handler.rs` (max 200 lines)
  ```rust
  use axum::{
      extract::State,
      http::StatusCode,
      Json,
  };
  use tracing::{debug, warn, info};
  use crate::api::http_server::AppState;
  use super::request::SearchApiRequest;
  use super::response::SearchApiResponse;

  /// POST /v1/search - Perform web search
  ///
  /// # Request
  /// - `query`: Search query string (required, max 500 chars)
  /// - `numResults`: Number of results (1-20, default 10)
  /// - `chainId`: Chain ID for billing (default 84532)
  ///
  /// # Response
  /// - `results`: Array of search results
  /// - `searchTimeMs`: Time taken for search
  /// - `provider`: Search provider used
  /// - `cached`: Whether result was from cache
  ///
  /// # Errors
  /// - 400 Bad Request: Invalid query or parameters
  /// - 503 Service Unavailable: Search disabled or no providers
  /// - 429 Too Many Requests: Rate limited
  pub async fn search_handler(
      State(state): State<AppState>,
      Json(request): Json<SearchApiRequest>,
  ) -> Result<Json<SearchApiResponse>, (StatusCode, String)> {
      debug!("Search request: {:?}", request.query);

      // Validate request
      if let Err(e) = request.validate() {
          warn!("Search validation failed: {}", e);
          return Err((StatusCode::BAD_REQUEST, e));
      }

      // Get search service
      let search_service = state.search_service.read().await;
      let search_service = search_service.as_ref().ok_or_else(|| {
          (StatusCode::SERVICE_UNAVAILABLE, "Search service not available".to_string())
      })?;

      if !search_service.is_enabled() {
          return Err((
              StatusCode::SERVICE_UNAVAILABLE,
              "Web search is disabled on this host".to_string(),
          ));
      }

      // Perform search
      let result = search_service
          .search(&request.query, Some(request.num_results))
          .await
          .map_err(|e| {
              match &e {
                  crate::search::types::SearchError::RateLimited { .. } => {
                      (StatusCode::TOO_MANY_REQUESTS, e.to_string())
                  }
                  crate::search::types::SearchError::SearchDisabled => {
                      (StatusCode::SERVICE_UNAVAILABLE, e.to_string())
                  }
                  _ => {
                      (StatusCode::INTERNAL_SERVER_ERROR, e.to_string())
                  }
              }
          })?;

      info!(
          "Search complete: {} results for '{}' in {}ms (cached: {})",
          result.result_count,
          request.query,
          result.search_time_ms,
          result.cached
      );

      Ok(Json(SearchApiResponse::new(
          result.query,
          result.results,
          result.search_time_ms,
          result.provider,
          result.cached,
          request.chain_id,
      )))
  }
  ```

---

### Sub-phase 4.3: Update AppState âœ…

**Goal**: Add SearchService to AppState

**Status**: COMPLETE (2025-01-05) - Implemented during Phase 1

#### Tasks
- [x] Add `search_service` field to AppState
- [x] Add setter method for search service
- [x] Update AppState::new_for_test() to include search service
- [x] Add /v1/search route to create_app()

**Implementation Files:**
- `src/api/http_server.rs` (modify)
  ```rust
  #[derive(Clone)]
  pub struct AppState {
      // ... existing fields ...
      pub search_service: Arc<RwLock<Option<Arc<SearchService>>>>,
  }
  ```

- `src/api/server.rs` (modify)
  ```rust
  impl ApiServer {
      pub async fn set_search_service(&self, service: Arc<SearchService>) {
          let mut guard = self.state.search_service.write().await;
          *guard = Some(service);
      }
  }
  ```

---

### Sub-phase 4.4: Chat Integration âœ…

**Goal**: Add web_search flag to chat/inference requests

**Status**: COMPLETE (2025-01-05)

#### Tasks
- [x] Write tests for chat with web_search=false (2 tests)
- [x] Write tests for chat with web_search=true (3 tests)
- [x] Write tests for search result injection into prompt (2 tests)
- [x] Add `web_search` field to InferenceRequest
- [x] Add `max_searches` field to InferenceRequest
- [x] Add `search_queries` field to InferenceRequest (optional custom queries)
- [x] Add search metadata to InferenceResponse (`web_search_performed`, `search_queries_count`, `search_provider`)
- [x] Modify inference handler to perform search before inference
- [x] Implement prompt augmentation with search results

**Files Modified:**
- `src/api/handlers.rs` - Added web_search, max_searches, search_queries to InferenceRequest; Added search metadata to InferenceResponse
- `src/api/http_server.rs` - inference_handler now performs web search when web_search=true
- `src/api/server.rs` - Updated InferenceResponse initialization
- `src/api/response_formatter.rs` - Updated test to include new fields

**Implementation Files:**
- `src/api/chat/request.rs` (modify)
  ```rust
  pub struct ChatRequest {
      // ... existing fields ...

      /// Enable web search before inference
      #[serde(default)]
      pub web_search: bool,

      /// Maximum searches to perform (default 5)
      #[serde(default = "default_max_searches")]
      pub max_searches: u32,

      /// Custom search queries (optional, auto-extracted if not provided)
      #[serde(skip_serializing_if = "Option::is_none")]
      pub search_queries: Option<Vec<String>>,
  }
  ```

---

## Phase 5: WebSocket Integration (2 hours) âœ… COMPLETE

### Sub-phase 5.1: WebSocket Message Types âœ…

**Goal**: Add search-related WebSocket messages

**Status**: COMPLETE (2025-01-05)

#### Tasks
- [x] Write tests for SearchRequest message serialization (2 tests)
- [x] Write tests for SearchResults message serialization (2 tests)
- [x] Write tests for SearchError message serialization (2 tests)
- [x] Add `SearchRequest` message type to MessageType enum
- [x] Add `WebSearchRequest` struct with validation
- [x] Add `WebSearchStarted` notification struct
- [x] Add `WebSearchResults` response struct
- [x] Add `WebSearchError` error struct with error codes
- [x] Add `WebSearchErrorCode` enum (SearchDisabled, RateLimited, InvalidQuery, etc.)

**Files Modified:**
- `src/api/websocket/message_types.rs` - Added MessageType variants and message structs

**Test Files:**
- Inline tests in `src/api/websocket/message_types.rs` (4 tests)

**Implementation Files:**
- `src/api/websocket/messages.rs` (modify)
  ```rust
  #[derive(Debug, Serialize, Deserialize)]
  #[serde(tag = "type")]
  pub enum ClientMessage {
      // ... existing variants ...

      /// Request a web search
      SearchRequest {
          query: String,
          num_results: Option<u32>,
          request_id: String,
      },

      /// Chat with web search enabled
      ChatWithSearch {
          messages: Vec<Message>,
          web_search: bool,
          max_searches: Option<u32>,
          request_id: String,
      },
  }

  #[derive(Debug, Serialize, Deserialize)]
  #[serde(tag = "type")]
  pub enum ServerMessage {
      // ... existing variants ...

      /// Search results
      SearchResults {
          request_id: String,
          query: String,
          results: Vec<SearchResult>,
          search_time_ms: u64,
          provider: String,
          cached: bool,
      },

      /// Search started notification
      SearchStarted {
          request_id: String,
          query: String,
      },

      /// Search error
      SearchError {
          request_id: String,
          error: String,
      },
  }
  ```

---

### Sub-phase 5.2: WebSocket Handler Integration

**Goal**: Handle search messages in WebSocket

**Status**: DEFERRED - Message types ready, handler integration can be added when needed

#### Tasks
- [ ] Add search request handling to WebSocket message handler
- [ ] Send SearchStarted before search
- [ ] Send SearchResults or SearchError after completion

**Note**: The message types are now complete. Handler integration can be added when SDK support for WebSocket search is implemented.

---

## Phase 6: Deep Research Agent (4 hours) - OPTIONAL

### Sub-phase 6.1: Research Module Structure

**Goal**: Create research agent module structure

**Status**: NOT STARTED (OPTIONAL PHASE)

#### Tasks
- [ ] Create `src/research/mod.rs`
- [ ] Create `src/research/agent.rs` stub
- [ ] Create `src/research/planner.rs` stub
- [ ] Create `src/research/synthesizer.rs` stub
- [ ] Add `pub mod research;` to `src/lib.rs`

---

### Sub-phase 6.2: Research Planner

**Goal**: Use LLM to generate research plans

**Status**: NOT STARTED (OPTIONAL PHASE)

#### Tasks
- [ ] Write tests for research plan generation (4 tests)
- [ ] Write tests for query extraction (3 tests)
- [ ] Implement `ResearchPlanner` struct
- [ ] Implement LLM prompt for query generation
- [ ] Parse LLM output into search queries

---

### Sub-phase 6.3: Research Agent

**Goal**: Implement agentic research loop

**Status**: NOT STARTED (OPTIONAL PHASE)

#### Tasks
- [ ] Write tests for research agent creation (2 tests)
- [ ] Write tests for single iteration (3 tests)
- [ ] Write tests for multi-iteration with gap detection (4 tests)
- [ ] Write tests for max iteration limit (2 tests)
- [ ] Implement `ResearchAgent` struct
- [ ] Implement research loop with progress callbacks
- [ ] Implement gap detection via LLM
- [ ] Implement final synthesis

---

### Sub-phase 6.4: Research API Endpoint

**Goal**: Implement POST /v1/research endpoint

**Status**: NOT STARTED (OPTIONAL PHASE)

#### Tasks
- [ ] Write tests for research endpoint (5 tests)
- [ ] Implement `research_handler` function
- [ ] Add streaming progress via WebSocket

---

## Phase 7: Integration & Documentation (2 hours)

### Sub-phase 7.1: Main.rs Integration

**Goal**: Initialize SearchService in main.rs

**Status**: NOT STARTED

#### Tasks
- [ ] Add SearchService initialization to main.rs
- [ ] Load SearchConfig from environment
- [ ] Log search availability at startup
- [ ] Handle missing API keys gracefully

**Implementation Files:**
- `src/main.rs` (modify)
  ```rust
  // Initialize Search Service (optional)
  println!("Initializing search service...");

  let search_config = SearchConfig::from_env();

  if search_config.enabled {
      match search_config.validate() {
          Ok(_) => {
              let search_service = SearchService::new(search_config);
              let providers = search_service.available_providers();
              println!("Search enabled with providers: {:?}", providers);
              api_server.set_search_service(Arc::new(search_service)).await;
          }
          Err(e) => {
              println!("Search configuration invalid: {}", e);
              println!("/v1/search will return 503");
          }
      }
  } else {
      println!("Web search disabled (set WEB_SEARCH_ENABLED=true to enable)");
  }
  ```

---

### Sub-phase 7.2: Update API Documentation

**Goal**: Document new endpoints in API.md

**Status**: NOT STARTED

#### Tasks
- [ ] Add POST /v1/search documentation
- [ ] Add web_search parameter to chat documentation
- [ ] Add WebSocket search message documentation
- [ ] Add configuration documentation
- [ ] Add request/response examples

---

### Sub-phase 7.3: Update Version

**Goal**: Update version information

**Status**: NOT STARTED

#### Tasks
- [ ] Update VERSION file to `8.7.0-web-search`
- [ ] Update src/version.rs with new version constants
- [ ] Add new features to FEATURES array (web-search, brave-search, research-agent)
- [ ] Update BREAKING_CHANGES if needed

---

### Sub-phase 7.4: Host Capability Advertisement

**Goal**: Advertise search capability to network

**Status**: NOT STARTED

#### Tasks
- [ ] Add `web_search_enabled` to host capabilities
- [ ] Add `search_providers` list to capabilities
- [ ] Update /v1/info endpoint with search status
- [ ] Document capability advertisement

---

## Test Summary

| Phase | Test File | Test Count |
|-------|-----------|------------|
| 1.3 | types.rs | ~12 |
| 1.4 | config.rs | ~11 |
| 2.1 | provider.rs | ~4 |
| 2.2 | brave.rs | ~15 |
| 2.3 | duckduckgo.rs | ~8 |
| 2.4 | bing.rs | ~9 |
| 3.1 | cache.rs | ~11 |
| 3.2 | rate_limiter.rs | ~8 |
| 3.3 | service.rs | ~17 |
| 4.1 | request.rs, response.rs | ~12 |
| 4.2 | test_search_endpoint.rs | ~11 |
| 4.4 | chat integration | ~7 |
| 5.1-5.2 | websocket | ~11 |
| 6.x | research (optional) | ~18 |
| **Total** | | **~144 tests** |

---

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| Single search | <2s | Including network latency |
| Cached search | <10ms | In-memory cache hit |
| Batch search (10 queries) | <5s | Parallel execution |
| Deep research (50 searches) | <60s | With synthesis |

---

## Security Considerations

| Concern | Mitigation |
|---------|------------|
| API key exposure | Store in environment variables, never log |
| Search query injection | Validate and sanitize queries |
| Rate limit abuse | Per-session and global rate limits |
| Cost attacks | Max searches per request/session limits |
| Privacy | Use privacy-respecting providers (Brave, DDG) |

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Brave API rate limits | Fallback to DuckDuckGo |
| Search API costs | Aggressive caching, rate limiting |
| Provider unavailability | Multiple provider failover |
| Large result payloads | Limit snippet length, result count |
| Slow searches | Timeout handling, graceful degradation |

---

## Pricing Integration Notes

Web searches should be factored into job pricing:

```rust
// Suggested pricing model
pub struct SearchPricing {
    /// Token equivalent per search (e.g., 100 tokens)
    pub tokens_per_search: u64,

    /// Whether search cost is separate or included in token price
    pub separate_billing: bool,
}

// In job completion
pub struct JobMetrics {
    pub tokens_generated: u64,
    pub searches_performed: u32,
    pub total_token_equivalent: u64,  // tokens + (searches * tokens_per_search)
}
```

---

## Phase 8: Streaming Web Search (2 hours) ğŸ”§ IN PROGRESS

### Problem Statement

Web search is only implemented in the **non-streaming HTTP inference path**. The streaming paths (both HTTP streaming and WebSocket) ignore the `web_search` flag completely.

| Path | Web Search Works |
|------|------------------|
| HTTP `/v1/inference` non-streaming | âœ… YES |
| HTTP `/v1/inference` streaming | âŒ NO |
| WebSocket `/v1/ws` streaming | âŒ NO |

The SDK uses WebSocket encrypted streaming, which is why web search never triggers for SDK users.

### Root Cause

In `/workspace/src/api/server.rs`, the `handle_streaming_request()` function (line ~690) **ignores the `web_search` flag** and uses the plain prompt directly:

```rust
// Line 724 - uses prompt WITHOUT search context
let full_prompt = build_prompt_with_context(&request.conversation_context, &request.prompt);
```

Compare to `handle_inference_request()` which has ~70 lines of web search logic (lines 497-566).

### Solution

Add web search execution to `handle_streaming_request()` before building the prompt, mirroring the logic in `handle_inference_request()`.

---

### Sub-phase 8.1: Add Tests for Streaming Web Search

**Goal**: Write tests for streaming web search integration (TDD approach)

**Status**: NOT STARTED

#### Tasks
- [ ] Write test `test_streaming_request_with_web_search_enabled`
- [ ] Write test `test_streaming_request_web_search_disabled_by_default`
- [ ] Write test `test_streaming_request_custom_search_queries`
- [ ] Write test `test_streaming_request_search_context_in_prompt`

**Test File**: `tests/api/test_streaming_search.rs` (new file)

```rust
//! Tests for web search in streaming inference

use fabstir_llm_node::api::handlers::InferenceRequest;
// ... test setup imports ...

#[tokio::test]
async fn test_streaming_request_with_web_search_enabled() {
    // Create request with web_search: true
    let request = InferenceRequest {
        prompt: "What is the latest news about AI?".to_string(),
        web_search: true,
        max_searches: 5,
        search_queries: None,
        stream: true,
        // ... other fields ...
    };

    // Verify search is performed before streaming starts
    // Verify search context is prepended to prompt
}

#[tokio::test]
async fn test_streaming_request_web_search_disabled_by_default() {
    let request = InferenceRequest {
        prompt: "Hello world".to_string(),
        web_search: false, // default
        stream: true,
        // ... other fields ...
    };

    // Verify no search is performed
    // Verify prompt is used as-is
}

#[tokio::test]
async fn test_streaming_request_custom_search_queries() {
    let request = InferenceRequest {
        prompt: "Tell me about these topics".to_string(),
        web_search: true,
        search_queries: Some(vec![
            "Rust programming 2025".to_string(),
            "WebAssembly trends".to_string(),
        ]),
        stream: true,
        // ... other fields ...
    };

    // Verify custom queries are used instead of auto-extraction
}

#[tokio::test]
async fn test_streaming_request_search_context_in_prompt() {
    let request = InferenceRequest {
        prompt: "Summarize the search results".to_string(),
        web_search: true,
        stream: true,
        // ... other fields ...
    };

    // Verify search results are prepended in format:
    // [Web Search Results]
    // - Title (URL): Snippet
    // [End Web Search Results]
    //
    // <original prompt>
}
```

---

### Sub-phase 8.2: Implement Streaming Web Search

**Goal**: Add web search logic to `handle_streaming_request()` in server.rs

**Status**: âœ… COMPLETE (2026-01-05)

#### Tasks
- [x] Copy web search logic from `handle_inference_request()` to `handle_streaming_request()`
- [x] Add search context prepending to prompt before streaming
- [x] Add logging for streaming web search
- [ ] Run tests from Sub-phase 8.1 to verify

**Implementation File**: `src/api/server.rs`

Add the following code to `handle_streaming_request()` BEFORE the `build_prompt_with_context()` call:

```rust
pub async fn handle_streaming_request(
    &self,
    request: InferenceRequest,
    client_ip: String,
) -> Result<mpsc::Receiver<StreamingResponse>, ApiError> {
    // ... existing validation ...

    // === ADD WEB SEARCH HERE (before building prompt) ===
    let mut search_context = String::new();

    if request.web_search {
        info!("ğŸ” Web search requested for streaming inference");

        let search_service_guard = self.search_service.read().await;
        if let Some(search_service) = search_service_guard.as_ref() {
            if search_service.is_enabled() {
                // Extract queries
                let queries = if let Some(ref custom_queries) = request.search_queries {
                    custom_queries.clone()
                } else {
                    let query = if request.prompt.len() > 200 {
                        request.prompt.chars().take(200).collect()
                    } else {
                        request.prompt.clone()
                    };
                    vec![query]
                };

                let max_searches = std::cmp::min(request.max_searches, 20) as usize;
                let queries_to_search: Vec<_> = queries.into_iter().take(max_searches).collect();

                // Perform searches
                let mut all_results = Vec::new();
                for query in &queries_to_search {
                    if let Ok(result) = search_service.search(query, Some(5)).await {
                        for sr in result.results {
                            all_results.push(format!("- {} ({}): {}", sr.title, sr.url, sr.snippet));
                        }
                    }
                }

                if !all_results.is_empty() {
                    search_context = format!(
                        "\n[Web Search Results]\n{}\n[End Web Search Results]\n\n",
                        all_results.join("\n")
                    );
                    info!("ğŸ” Web search completed for streaming: {} results", all_results.len());
                }
            } else {
                warn!("ğŸ” Web search requested but search service is disabled");
            }
        } else {
            warn!("ğŸ” Web search requested but search service is not configured");
        }
    }

    // Build prompt WITH search context
    let prompt_with_search = if !search_context.is_empty() {
        format!("{}{}", search_context, request.prompt)
    } else {
        request.prompt.clone()
    };
    let full_prompt = build_prompt_with_context(&request.conversation_context, &prompt_with_search);

    // ... rest of function unchanged ...
}
```

---

### Sub-phase 8.3: WebSocket Streaming Integration

**Goal**: Ensure WebSocket path also triggers web search

**Status**: NOT STARTED

#### Tasks
- [ ] Verify WebSocket messages use `handle_streaming_request()` internally
- [ ] Write test `test_websocket_inference_with_web_search`
- [ ] Add `web_search` field to WebSocket inference message type (if not present)
- [ ] Run integration test with WebSocket client

**Test File**: `tests/websocket/test_websocket_search.rs` (new file)

```rust
//! Tests for web search in WebSocket streaming

#[tokio::test]
async fn test_websocket_inference_with_web_search() {
    // Connect to WebSocket
    // Send inference message with web_search: true
    // Verify search is performed
    // Verify streaming response includes search context
}
```

**Note**: If WebSocket uses a separate code path, we may need to add web search logic there as well.

---

### Sub-phase 8.4: Update Version

**Goal**: Update version to v8.7.5-web-search

**Status**: âœ… COMPLETE (2026-01-05)

#### Tasks
- [x] Update `/workspace/VERSION` to `8.7.5-web-search`
- [x] Update `/workspace/src/version.rs`:
  - [x] VERSION constant
  - [x] VERSION_NUMBER
  - [x] VERSION_PATCH
  - [x] Test assertions
- [ ] Build binary: `cargo build --release --features real-ezkl -j 4`
- [ ] Verify version in binary: `strings target/release/fabstir-llm-node | grep "v8.7.5"`

---

### Sub-phase 8.5: Integration Testing

**Goal**: End-to-end testing of streaming web search

**Status**: NOT STARTED

#### Tasks
- [ ] Run all search-related tests: `cargo test search`
- [ ] Run streaming tests: `cargo test streaming`
- [ ] Test with curl (streaming):
  ```bash
  curl -X POST http://localhost:8080/v1/inference \
    -H 'Content-Type: application/json' \
    -d '{
      "prompt": "What are the latest NVIDIA GPU specs?",
      "web_search": true,
      "stream": true
    }'
  ```
- [ ] Test with SDK (WebSocket):
  ```typescript
  const response = await client.inference({
    prompt: "Use web search to find the latest AI news",
    web_search: true,
    stream: true
  });
  ```
- [ ] Verify logs show:
  ```
  ğŸ” Web search requested for streaming inference
  ğŸ” Web search completed for streaming: X results
  ğŸ“Š Calling checkpoint_manager.track_tokens for job N
  âœ… Sent encrypted_chunk 1 (tokens: 1)
  ```

---

### Expected Log Output After Fix

```
ğŸ” Web search requested for streaming inference
ğŸ” Web search completed for streaming: 5 results
ğŸ“Š Calling checkpoint_manager.track_tokens for job 102
...
âœ… Sent encrypted_chunk 1 (tokens: 1)
âœ… Sent encrypted_chunk 2 (tokens: 1)
...
```

---

### Risk Assessment

| Risk | Level | Mitigation |
|------|-------|------------|
| Adding latency before streaming starts | Low | Search adds 1-2s before first token; acceptable for search use case |
| Backwards compatibility | None | `web_search: false` (default) = no change in behavior |
| Memory usage | Low | Search context is typically <10KB |
| Provider failures | Low | Existing failover logic handles this gracefully |

---

### Files Modified in Phase 8

| File | Change |
|------|--------|
| `src/api/server.rs` | Add web search to `handle_streaming_request()` |
| `tests/api/test_streaming_search.rs` | New test file |
| `tests/websocket/test_websocket_search.rs` | New test file |
| `VERSION` | Update to 8.7.5-web-search |
| `src/version.rs` | Update version constants |

---

### Note on "Excessive tokens claimed" Error

The separate error in the logs:
```
âŒ execution reverted: Excessive tokens claimed
```

This is **unrelated to web search**. It means the job on-chain has a token limit lower than the tokens generated. This is a job configuration issue, not a code bug.

---

## Phase 9: Content Fetching (4 hours) âœ… COMPLETE

### Problem Statement

Current web search returns **metadata only** (title, URL, snippet/meta description). The LLM cannot answer questions about actual web content because it never sees the content - only links.

**Current behavior:**
```
User: "Search for latest news about AI"
Search returns: BBC News (https://bbc.com/news) - "BBC News brings you coverage..."
LLM responds: "I can't browse the web, but you can check BBC News..."
```

**Desired behavior:**
```
User: "Search for latest news about AI"
Search returns: URLs â†’ Node fetches pages â†’ Extracts content
LLM sees: "OpenAI announced GPT-5 development is underway..."
LLM responds: "According to recent news, OpenAI announced..."
```

### Solution Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Search Pipeline (Current)                   â”‚
â”‚  Query â†’ Search API â†’ URLs + Snippets â†’ Inject into prompt      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Content Fetching (New Phase 9)                 â”‚
â”‚                                                                  â”‚
â”‚  URLs from Search                                                â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ ContentFetcher  â”‚ â”€â”€ HTTP GET (parallel, 3 pages max)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                      â”‚
â”‚           â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ ContentExtractorâ”‚ â”€â”€ CSS selectors: article, main, .content  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                      â”‚
â”‚           â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ ContentCache    â”‚ â”€â”€ TTL: 30 minutes, max 500 entries        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                      â”‚
â”‚           â–¼                                                      â”‚
â”‚  Actual page content (3000 chars/page, 8000 chars total)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM Prompt                                  â”‚
â”‚  [Web Search Results]                                            â”‚
â”‚  [1] BBC News - AI Developments                                  â”‚
â”‚  URL: https://bbc.com/news/ai                                    â”‚
â”‚  Content:                                                        â”‚
â”‚  OpenAI announced today that GPT-5 development is underway...   â”‚
â”‚  ---                                                             â”‚
â”‚  [2] Reuters - Tech News                                         â”‚
â”‚  ...                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Structure

```
src/search/
â”œâ”€â”€ mod.rs                    # Add content module export
â”œâ”€â”€ content/                  # NEW MODULE
â”‚   â”œâ”€â”€ mod.rs               # Public exports
â”‚   â”œâ”€â”€ fetcher.rs           # HTTP fetching with timeouts
â”‚   â”œâ”€â”€ extractor.rs         # HTML â†’ text extraction
â”‚   â”œâ”€â”€ cache.rs             # Content caching (30min TTL)
â”‚   â””â”€â”€ config.rs            # ContentFetchConfig
```

### Dependencies Required

```toml
# Cargo.toml additions
scraper = "0.18"              # HTML parsing with CSS selectors
html-escape = "0.2"           # HTML entity decoding (optional)
```

**Note**: `reqwest` already present for HTTP requests.

### Environment Variables

```bash
# Content Fetching Configuration (new)
CONTENT_FETCH_ENABLED=true              # Enable/disable content fetching
CONTENT_FETCH_MAX_PAGES=3               # Max pages to fetch per search (1-5)
CONTENT_FETCH_MAX_CHARS_PER_PAGE=3000   # Max characters per page
CONTENT_FETCH_MAX_TOTAL_CHARS=8000      # Max total characters for all pages
CONTENT_FETCH_TIMEOUT_SECS=10           # Total timeout for all fetches
CONTENT_FETCH_TIMEOUT_PER_PAGE_SECS=5   # Timeout per individual page
CONTENT_FETCH_CACHE_TTL_SECS=1800       # Cache TTL (30 minutes)
```

---

### Sub-phase 9.1: Add Dependencies

**Goal**: Add scraper crate for HTML parsing

**Status**: âœ… COMPLETE

#### Tasks
- [x] Add `scraper = "0.18"` to Cargo.toml
- [x] Run `cargo check` to verify dependency compiles
- [x] Run `cargo test --lib` to ensure no regressions

**Implementation Files:**
- `Cargo.toml` - Add dependency

---

### Sub-phase 9.2: Create Module Structure

**Goal**: Create stub files for content fetching module

**Status**: âœ… COMPLETE

#### Tasks
- [x] Create `src/search/content/mod.rs` with submodule declarations
- [x] Create `src/search/content/config.rs` with ContentFetchConfig
- [x] Create `src/search/content/fetcher.rs` with ContentFetcher stub
- [x] Create `src/search/content/extractor.rs` with extract_main_content stub
- [x] Create `src/search/content/cache.rs` with ContentCache stub
- [x] Add `pub mod content;` to `src/search/mod.rs`
- [x] Run `cargo check` to verify module structure

**Implementation Files:**
- `src/search/content/mod.rs`:
  ```rust
  //! Content fetching module for web search enhancement
  //!
  //! Fetches actual page content from search result URLs to provide
  //! LLM with real information instead of just snippets.

  pub mod config;
  pub mod fetcher;
  pub mod extractor;
  pub mod cache;

  pub use config::ContentFetchConfig;
  pub use fetcher::ContentFetcher;
  pub use cache::ContentCache;
  ```

---

### Sub-phase 9.3: Define Configuration (TDD)

**Goal**: Define content fetch configuration with environment variable loading

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_content_fetch_config_defaults` (verify sensible defaults)
- [x] Write test `test_content_fetch_config_from_env` (verify env loading)
- [x] Write test `test_content_fetch_config_validation` (verify bounds checking)
- [x] Implement `ContentFetchConfig` struct
- [x] Implement `from_env()` loading function
- [x] Implement `validate()` method

**Test File**: `src/search/content/config.rs` (inline tests)

```rust
//! Configuration for content fetching

use std::env;

#[derive(Debug, Clone)]
pub struct ContentFetchConfig {
    /// Enable content fetching (default: true when search enabled)
    pub enabled: bool,
    /// Maximum pages to fetch per search (default: 3)
    pub max_pages: usize,
    /// Maximum characters per page (default: 3000)
    pub max_chars_per_page: usize,
    /// Maximum total characters for all pages (default: 8000)
    pub max_total_chars: usize,
    /// Timeout per page fetch in seconds (default: 5)
    pub timeout_per_page_secs: u64,
    /// Total timeout for all fetches in seconds (default: 10)
    pub total_timeout_secs: u64,
    /// Cache TTL in seconds (default: 1800 = 30 minutes)
    pub cache_ttl_secs: u64,
    /// Maximum cache entries (default: 500)
    pub max_cache_entries: usize,
}

impl ContentFetchConfig {
    pub fn from_env() -> Self {
        Self {
            enabled: env::var("CONTENT_FETCH_ENABLED")
                .map(|v| v.to_lowercase() == "true")
                .unwrap_or(true), // Enabled by default when search is enabled
            max_pages: env::var("CONTENT_FETCH_MAX_PAGES")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(3)
                .min(5), // Cap at 5
            max_chars_per_page: env::var("CONTENT_FETCH_MAX_CHARS_PER_PAGE")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(3000),
            max_total_chars: env::var("CONTENT_FETCH_MAX_TOTAL_CHARS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(8000),
            timeout_per_page_secs: env::var("CONTENT_FETCH_TIMEOUT_PER_PAGE_SECS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(5),
            total_timeout_secs: env::var("CONTENT_FETCH_TIMEOUT_SECS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(10),
            cache_ttl_secs: env::var("CONTENT_FETCH_CACHE_TTL_SECS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(1800),
            max_cache_entries: 500,
        }
    }

    pub fn validate(&self) -> Result<(), String> {
        if self.max_pages == 0 {
            return Err("max_pages must be at least 1".to_string());
        }
        if self.max_chars_per_page < 100 {
            return Err("max_chars_per_page must be at least 100".to_string());
        }
        if self.timeout_per_page_secs == 0 {
            return Err("timeout_per_page_secs must be at least 1".to_string());
        }
        Ok(())
    }
}

impl Default for ContentFetchConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            max_pages: 3,
            max_chars_per_page: 3000,
            max_total_chars: 8000,
            timeout_per_page_secs: 5,
            total_timeout_secs: 10,
            cache_ttl_secs: 1800,
            max_cache_entries: 500,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_content_fetch_config_defaults() {
        let config = ContentFetchConfig::default();
        assert!(config.enabled);
        assert_eq!(config.max_pages, 3);
        assert_eq!(config.max_chars_per_page, 3000);
        assert_eq!(config.max_total_chars, 8000);
        assert_eq!(config.timeout_per_page_secs, 5);
        assert_eq!(config.total_timeout_secs, 10);
        assert_eq!(config.cache_ttl_secs, 1800);
    }

    #[test]
    fn test_content_fetch_config_validation() {
        let mut config = ContentFetchConfig::default();
        assert!(config.validate().is_ok());

        config.max_pages = 0;
        assert!(config.validate().is_err());

        config.max_pages = 3;
        config.max_chars_per_page = 50;
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_content_fetch_config_from_env() {
        // Test that from_env doesn't panic with no env vars
        let config = ContentFetchConfig::from_env();
        assert!(config.max_pages <= 5); // Should be capped
    }
}
```

---

### Sub-phase 9.4: Implement Content Extractor (TDD)

**Goal**: Extract main content from HTML using CSS selectors

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_extract_article_content` (basic article tag)
- [x] Write test `test_extract_main_content` (main tag)
- [x] Write test `test_extract_content_with_class` (common content classes)
- [x] Write test `test_extract_fallback_body` (fallback to body)
- [x] Write test `test_strip_scripts_and_styles` (remove noise)
- [x] Write test `test_clean_whitespace` (normalize spacing)
- [x] Write test `test_truncate_content` (respect max length)
- [x] Implement `extract_main_content()` function
- [x] Implement `clean_text()` helper
- [x] Implement `truncate_content()` helper

**Test File**: `src/search/content/extractor.rs` (inline tests)

```rust
//! HTML content extraction
//!
//! Extracts main content from web pages using CSS selectors.

use scraper::{Html, Selector};

/// Extract main content from HTML
///
/// Tries multiple strategies in order:
/// 1. <article> tag
/// 2. <main> tag
/// 3. [role="main"] attribute
/// 4. Common content class names (.content, .post-content, .article-body, etc.)
/// 5. Fallback to <body> with noise removal
///
/// # Arguments
/// * `html` - Raw HTML string
/// * `max_chars` - Maximum characters to return
///
/// # Returns
/// Extracted text content, cleaned and truncated
pub fn extract_main_content(html: &str, max_chars: usize) -> String {
    let document = Html::parse_document(html);

    // Priority order of selectors to try
    let selectors = [
        "article",
        "main",
        "[role='main']",
        ".post-content",
        ".article-content",
        ".entry-content",
        ".story-body",        // BBC
        ".article__body",     // News sites
        ".content-body",
        "#article-body",
        "#content",
        ".prose",             // Tailwind
    ];

    for selector_str in selectors {
        if let Ok(selector) = Selector::parse(selector_str) {
            if let Some(element) = document.select(&selector).next() {
                let text = extract_text_from_element(&element);
                let cleaned = clean_text(&text);
                if cleaned.len() > 200 {
                    // Found substantial content
                    return truncate_content(&cleaned, max_chars);
                }
            }
        }
    }

    // Fallback: extract from body, removing nav/footer/script
    extract_body_text(&document, max_chars)
}

/// Extract text from an HTML element, stripping tags
fn extract_text_from_element(element: &scraper::ElementRef) -> String {
    element
        .text()
        .collect::<Vec<_>>()
        .join(" ")
}

/// Extract text from body, removing common noise elements
fn extract_body_text(document: &Html, max_chars: usize) -> String {
    // Try to get body
    if let Ok(body_selector) = Selector::parse("body") {
        if let Some(body) = document.select(&body_selector).next() {
            let text = extract_text_from_element(&body);
            let cleaned = clean_text(&text);
            return truncate_content(&cleaned, max_chars);
        }
    }
    String::new()
}

/// Clean text: normalize whitespace, remove excess blank lines
fn clean_text(text: &str) -> String {
    text.split_whitespace()
        .collect::<Vec<_>>()
        .join(" ")
        .trim()
        .to_string()
}

/// Truncate content to max_chars, preserving word boundaries
fn truncate_content(text: &str, max_chars: usize) -> String {
    if text.len() <= max_chars {
        return text.to_string();
    }

    // Find last space before max_chars
    let truncated = &text[..max_chars];
    if let Some(last_space) = truncated.rfind(' ') {
        format!("{}...", &text[..last_space])
    } else {
        format!("{}...", truncated)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    const SAMPLE_HTML_ARTICLE: &str = r#"
        <!DOCTYPE html>
        <html>
        <head><title>Test</title></head>
        <body>
            <nav>Navigation links here</nav>
            <article>
                <h1>Main Article Title</h1>
                <p>This is the main content of the article with important information.</p>
                <p>More substantial content that should be extracted.</p>
            </article>
            <footer>Footer content</footer>
        </body>
        </html>
    "#;

    const SAMPLE_HTML_MAIN: &str = r#"
        <!DOCTYPE html>
        <html>
        <body>
            <header>Site Header</header>
            <main>
                <h1>Page Title</h1>
                <p>Main content goes here with detailed information about the topic.</p>
            </main>
            <aside>Sidebar content</aside>
        </body>
        </html>
    "#;

    const SAMPLE_HTML_CLASS: &str = r#"
        <!DOCTYPE html>
        <html>
        <body>
            <div class="post-content">
                <p>Blog post content with enough text to be considered substantial.</p>
                <p>Additional paragraph with more content for the reader.</p>
            </div>
        </body>
        </html>
    "#;

    #[test]
    fn test_extract_article_content() {
        let content = extract_main_content(SAMPLE_HTML_ARTICLE, 3000);
        assert!(content.contains("Main Article Title"));
        assert!(content.contains("main content"));
        assert!(!content.contains("Navigation"));
        assert!(!content.contains("Footer"));
    }

    #[test]
    fn test_extract_main_content() {
        let content = extract_main_content(SAMPLE_HTML_MAIN, 3000);
        assert!(content.contains("Page Title"));
        assert!(content.contains("Main content"));
        assert!(!content.contains("Site Header"));
        assert!(!content.contains("Sidebar"));
    }

    #[test]
    fn test_extract_content_with_class() {
        let content = extract_main_content(SAMPLE_HTML_CLASS, 3000);
        assert!(content.contains("Blog post content"));
    }

    #[test]
    fn test_clean_whitespace() {
        let dirty = "  Hello   world  \n\n  test  ";
        let cleaned = clean_text(dirty);
        assert_eq!(cleaned, "Hello world test");
    }

    #[test]
    fn test_truncate_content() {
        let long_text = "This is a long text that needs to be truncated at word boundary";
        let truncated = truncate_content(long_text, 30);
        assert!(truncated.len() <= 33); // 30 + "..."
        assert!(truncated.ends_with("..."));
        assert!(!truncated.contains("truncated")); // Word boundary
    }

    #[test]
    fn test_truncate_short_content() {
        let short = "Short text";
        let result = truncate_content(short, 100);
        assert_eq!(result, "Short text"); // No truncation needed
    }
}
```

---

### Sub-phase 9.5: Implement Content Cache (TDD)

**Goal**: Cache fetched content to reduce latency and bandwidth

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_cache_insert_and_get` (basic operations)
- [x] Write test `test_cache_ttl_expiration` (expired entries not returned)
- [x] Write test `test_cache_max_entries` (eviction when full)
- [x] Write test `test_cache_key_normalization` (URL normalization)
- [x] Implement `ContentCache` struct
- [x] Implement `get()`, `insert()`, `clear()` methods
- [x] Implement TTL checking
- [x] Implement LRU-style eviction

**Test File**: `src/search/content/cache.rs` (inline tests)

```rust
//! Content caching for fetched web pages

use std::collections::HashMap;
use std::sync::RwLock;
use std::time::{Duration, Instant};

/// Cached page content
#[derive(Debug, Clone)]
pub struct CachedContent {
    pub url: String,
    pub title: String,
    pub text: String,
    pub fetched_at: Instant,
}

/// Content cache with TTL-based expiration
pub struct ContentCache {
    cache: RwLock<HashMap<String, CachedContent>>,
    ttl: Duration,
    max_entries: usize,
}

impl ContentCache {
    pub fn new(ttl_secs: u64, max_entries: usize) -> Self {
        Self {
            cache: RwLock::new(HashMap::new()),
            ttl: Duration::from_secs(ttl_secs),
            max_entries,
        }
    }

    /// Get cached content if not expired
    pub fn get(&self, url: &str) -> Option<CachedContent> {
        let cache = self.cache.read().ok()?;
        let key = Self::normalize_url(url);
        let entry = cache.get(&key)?;

        if entry.fetched_at.elapsed() > self.ttl {
            return None; // Expired
        }

        Some(entry.clone())
    }

    /// Insert content into cache
    pub fn insert(&self, url: &str, title: String, text: String) {
        let mut cache = match self.cache.write() {
            Ok(c) => c,
            Err(_) => return,
        };

        // Evict oldest if at capacity
        if cache.len() >= self.max_entries {
            self.evict_oldest(&mut cache);
        }

        let key = Self::normalize_url(url);
        cache.insert(key, CachedContent {
            url: url.to_string(),
            title,
            text,
            fetched_at: Instant::now(),
        });
    }

    /// Clear all cached entries
    pub fn clear(&self) {
        if let Ok(mut cache) = self.cache.write() {
            cache.clear();
        }
    }

    /// Get cache statistics
    pub fn stats(&self) -> ContentCacheStats {
        let cache = self.cache.read().unwrap();
        let total = cache.len();
        let expired = cache.values()
            .filter(|e| e.fetched_at.elapsed() > self.ttl)
            .count();
        ContentCacheStats { total, expired, max: self.max_entries }
    }

    /// Normalize URL for cache key (lowercase, remove trailing slash)
    fn normalize_url(url: &str) -> String {
        url.to_lowercase()
            .trim_end_matches('/')
            .to_string()
    }

    fn evict_oldest(&self, cache: &mut HashMap<String, CachedContent>) {
        if let Some(oldest_key) = cache.iter()
            .min_by_key(|(_, v)| v.fetched_at)
            .map(|(k, _)| k.clone())
        {
            cache.remove(&oldest_key);
        }
    }
}

#[derive(Debug)]
pub struct ContentCacheStats {
    pub total: usize,
    pub expired: usize,
    pub max: usize,
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::thread::sleep;

    #[test]
    fn test_cache_insert_and_get() {
        let cache = ContentCache::new(3600, 100);

        cache.insert(
            "https://example.com/page",
            "Example Title".to_string(),
            "Example content".to_string(),
        );

        let result = cache.get("https://example.com/page");
        assert!(result.is_some());

        let content = result.unwrap();
        assert_eq!(content.title, "Example Title");
        assert_eq!(content.text, "Example content");
    }

    #[test]
    fn test_cache_ttl_expiration() {
        let cache = ContentCache::new(1, 100); // 1 second TTL

        cache.insert(
            "https://example.com/expire",
            "Title".to_string(),
            "Content".to_string(),
        );

        // Should exist immediately
        assert!(cache.get("https://example.com/expire").is_some());

        // Wait for expiration
        sleep(Duration::from_secs(2));

        // Should be expired
        assert!(cache.get("https://example.com/expire").is_none());
    }

    #[test]
    fn test_cache_key_normalization() {
        let cache = ContentCache::new(3600, 100);

        cache.insert(
            "https://Example.COM/Page/",
            "Title".to_string(),
            "Content".to_string(),
        );

        // Should match with different case/trailing slash
        assert!(cache.get("https://example.com/page").is_some());
        assert!(cache.get("HTTPS://EXAMPLE.COM/PAGE/").is_some());
    }

    #[test]
    fn test_cache_max_entries() {
        let cache = ContentCache::new(3600, 3); // Max 3 entries

        for i in 0..5 {
            cache.insert(
                &format!("https://example.com/{}", i),
                format!("Title {}", i),
                format!("Content {}", i),
            );
        }

        let stats = cache.stats();
        assert!(stats.total <= 3);
    }
}
```

---

### Sub-phase 9.6: Implement Content Fetcher (TDD)

**Goal**: HTTP fetching with parallel requests and timeouts

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_fetcher_creation` (verify config applied)
- [x] Write test `test_fetch_single_url` (mock server)
- [x] Write test `test_fetch_timeout` (respects timeout)
- [x] Write test `test_fetch_parallel` (multiple URLs)
- [x] Write test `test_fetch_with_cache_hit` (returns cached)
- [x] Write test `test_fetch_invalid_url` (graceful error)
- [x] Write test `test_is_safe_url` (blocks localhost/private IPs)
- [x] Implement `ContentFetcher` struct
- [x] Implement `fetch_content()` for single URL
- [x] Implement `fetch_multiple()` for parallel fetching
- [x] Implement URL safety validation
- [x] Implement cache integration

**Test File**: `src/search/content/fetcher.rs` (inline tests)

```rust
//! HTTP content fetching with parallel requests and timeouts

use std::sync::Arc;
use std::time::Duration;
use reqwest::Client;
use tracing::{debug, warn, info};
use url::Url;

use super::cache::{ContentCache, CachedContent};
use super::config::ContentFetchConfig;
use super::extractor::extract_main_content;

/// Fetched page content
#[derive(Debug, Clone)]
pub struct PageContent {
    pub url: String,
    pub title: String,
    pub text: String,
}

/// Content fetcher with caching and parallel requests
pub struct ContentFetcher {
    client: Client,
    cache: Arc<ContentCache>,
    config: ContentFetchConfig,
}

impl ContentFetcher {
    pub fn new(config: ContentFetchConfig) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_per_page_secs))
            .user_agent("Mozilla/5.0 (compatible; FabstirBot/1.0; +https://fabstir.com)")
            .redirect(reqwest::redirect::Policy::limited(5))
            .build()
            .expect("Failed to create HTTP client");

        let cache = Arc::new(ContentCache::new(
            config.cache_ttl_secs,
            config.max_cache_entries,
        ));

        Self { client, cache, config }
    }

    /// Fetch content from a single URL
    pub async fn fetch_content(&self, url: &str) -> Result<PageContent, FetchError> {
        // Validate URL safety
        if !Self::is_safe_url(url) {
            return Err(FetchError::UnsafeUrl(url.to_string()));
        }

        // Check cache first
        if let Some(cached) = self.cache.get(url) {
            debug!("Content cache hit for: {}", url);
            return Ok(PageContent {
                url: cached.url,
                title: cached.title,
                text: cached.text,
            });
        }

        debug!("Fetching content from: {}", url);

        // Fetch page
        let response = self.client
            .get(url)
            .send()
            .await
            .map_err(|e| {
                if e.is_timeout() {
                    FetchError::Timeout(url.to_string())
                } else {
                    FetchError::HttpError(e.to_string())
                }
            })?;

        let status = response.status();
        if !status.is_success() {
            return Err(FetchError::HttpStatus(status.as_u16(), url.to_string()));
        }

        let html = response.text().await
            .map_err(|e| FetchError::HttpError(e.to_string()))?;

        // Extract content
        let text = extract_main_content(&html, self.config.max_chars_per_page);

        if text.len() < 100 {
            return Err(FetchError::NoContent(url.to_string()));
        }

        // Extract title from HTML
        let title = Self::extract_title(&html).unwrap_or_else(|| url.to_string());

        // Cache the result
        self.cache.insert(url, title.clone(), text.clone());

        info!("Fetched {} chars from: {}", text.len(), url);

        Ok(PageContent { url: url.to_string(), title, text })
    }

    /// Fetch content from multiple URLs in parallel
    ///
    /// Returns results in same order as input URLs.
    /// Failed fetches return Err, but don't stop other fetches.
    pub async fn fetch_multiple(&self, urls: &[String]) -> Vec<Result<PageContent, FetchError>> {
        use futures::future::join_all;
        use tokio::time::timeout;

        let total_timeout = Duration::from_secs(self.config.total_timeout_secs);
        let max_pages = self.config.max_pages;

        // Limit number of URLs to fetch
        let urls_to_fetch: Vec<_> = urls.iter().take(max_pages).collect();

        let futures: Vec<_> = urls_to_fetch
            .iter()
            .map(|url| self.fetch_content(url))
            .collect();

        // Apply total timeout to all fetches
        match timeout(total_timeout, join_all(futures)).await {
            Ok(results) => results,
            Err(_) => {
                warn!("Total fetch timeout exceeded");
                vec![Err(FetchError::Timeout("total".to_string())); urls_to_fetch.len()]
            }
        }
    }

    /// Check if URL is safe to fetch (not localhost/private IP)
    fn is_safe_url(url: &str) -> bool {
        let parsed = match Url::parse(url) {
            Ok(u) => u,
            Err(_) => return false,
        };

        // Only allow http/https
        if !["http", "https"].contains(&parsed.scheme()) {
            return false;
        }

        // Block localhost and private IPs
        if let Some(host) = parsed.host_str() {
            let host_lower = host.to_lowercase();
            if host_lower == "localhost"
                || host_lower == "127.0.0.1"
                || host_lower.starts_with("192.168.")
                || host_lower.starts_with("10.")
                || host_lower.starts_with("172.16.")
                || host_lower.starts_with("172.17.")
                || host_lower.starts_with("172.18.")
                || host_lower.starts_with("0.0.0.0")
            {
                return false;
            }
        }

        true
    }

    /// Extract title from HTML
    fn extract_title(html: &str) -> Option<String> {
        use scraper::{Html, Selector};

        let document = Html::parse_document(html);
        let selector = Selector::parse("title").ok()?;

        document.select(&selector)
            .next()
            .map(|el| el.text().collect::<String>().trim().to_string())
    }

    /// Get cache statistics
    pub fn cache_stats(&self) -> super::cache::ContentCacheStats {
        self.cache.stats()
    }

    /// Check if content fetching is enabled
    pub fn is_enabled(&self) -> bool {
        self.config.enabled
    }
}

#[derive(Debug, Clone)]
pub enum FetchError {
    Timeout(String),
    HttpError(String),
    HttpStatus(u16, String),
    NoContent(String),
    UnsafeUrl(String),
}

impl std::fmt::Display for FetchError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Timeout(url) => write!(f, "Timeout fetching: {}", url),
            Self::HttpError(msg) => write!(f, "HTTP error: {}", msg),
            Self::HttpStatus(code, url) => write!(f, "HTTP {} for: {}", code, url),
            Self::NoContent(url) => write!(f, "No content extracted from: {}", url),
            Self::UnsafeUrl(url) => write!(f, "Unsafe URL blocked: {}", url),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_is_safe_url() {
        assert!(ContentFetcher::is_safe_url("https://example.com/page"));
        assert!(ContentFetcher::is_safe_url("http://bbc.com/news"));

        // Block unsafe URLs
        assert!(!ContentFetcher::is_safe_url("http://localhost/admin"));
        assert!(!ContentFetcher::is_safe_url("http://127.0.0.1:8080"));
        assert!(!ContentFetcher::is_safe_url("http://192.168.1.1/router"));
        assert!(!ContentFetcher::is_safe_url("http://10.0.0.1/internal"));
        assert!(!ContentFetcher::is_safe_url("ftp://example.com/file"));
        assert!(!ContentFetcher::is_safe_url("file:///etc/passwd"));
    }

    #[test]
    fn test_extract_title() {
        let html = "<html><head><title>Test Page Title</title></head><body></body></html>";
        let title = ContentFetcher::extract_title(html);
        assert_eq!(title, Some("Test Page Title".to_string()));
    }

    #[test]
    fn test_extract_title_missing() {
        let html = "<html><body>No title here</body></html>";
        let title = ContentFetcher::extract_title(html);
        assert!(title.is_none());
    }

    #[tokio::test]
    async fn test_fetcher_creation() {
        let config = ContentFetchConfig::default();
        let fetcher = ContentFetcher::new(config);
        assert!(fetcher.is_enabled());
    }

    // Note: Real HTTP tests should use a mock server (wiremock)
    // or be marked as integration tests that hit real URLs
}
```

---

### Sub-phase 9.7: Integrate with Search Service (TDD)

**Goal**: Connect ContentFetcher to SearchService pipeline

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_search_with_content_fetch_enabled`
- [x] Write test `test_search_with_content_fetch_disabled`
- [x] Write test `test_search_content_fallback_to_snippet`
- [x] Add `content_fetcher` field to `SearchService`
- [x] Modify `search()` to optionally fetch content
- [x] Add `search_with_content()` method
- [x] Update `format_results_for_prompt()` to include content

**Implementation File**: `src/search/service.rs` (modify)

```rust
// Add to SearchService struct
pub struct SearchService {
    // ... existing fields ...
    content_fetcher: Option<ContentFetcher>,
}

impl SearchService {
    // Add new method
    pub async fn search_with_content(
        &self,
        query: &str,
        num_results: Option<usize>,
    ) -> Result<SearchResponseWithContent, SearchError> {
        // 1. Perform regular search
        let search_response = self.search(query, num_results).await?;

        // 2. If content fetcher enabled, fetch page content
        if let Some(ref fetcher) = self.content_fetcher {
            if fetcher.is_enabled() {
                let urls: Vec<_> = search_response.results
                    .iter()
                    .map(|r| r.url.clone())
                    .collect();

                let contents = fetcher.fetch_multiple(&urls).await;

                // Combine search results with fetched content
                let results_with_content: Vec<_> = search_response.results
                    .into_iter()
                    .zip(contents.into_iter())
                    .map(|(result, content)| {
                        SearchResultWithContent {
                            title: result.title,
                            url: result.url,
                            snippet: result.snippet,
                            content: content.ok().map(|c| c.text),
                            source: result.source,
                        }
                    })
                    .collect();

                return Ok(SearchResponseWithContent {
                    query: search_response.query,
                    results: results_with_content,
                    search_time_ms: search_response.search_time_ms,
                    provider: search_response.provider,
                    cached: search_response.cached,
                });
            }
        }

        // Content fetching disabled - return snippet only
        Ok(SearchResponseWithContent {
            query: search_response.query,
            results: search_response.results.into_iter().map(|r| {
                SearchResultWithContent {
                    title: r.title,
                    url: r.url,
                    snippet: r.snippet.clone(),
                    content: None, // No content fetched
                    source: r.source,
                }
            }).collect(),
            search_time_ms: search_response.search_time_ms,
            provider: search_response.provider,
            cached: search_response.cached,
        })
    }
}

#[derive(Debug, Clone)]
pub struct SearchResultWithContent {
    pub title: String,
    pub url: String,
    pub snippet: String,
    pub content: Option<String>, // Actual page content if fetched
    pub source: String,
}

#[derive(Debug, Clone)]
pub struct SearchResponseWithContent {
    pub query: String,
    pub results: Vec<SearchResultWithContent>,
    pub search_time_ms: u64,
    pub provider: String,
    pub cached: bool,
}
```

---

### Sub-phase 9.8: Update Prompt Formatter

**Goal**: Format search results with content for LLM prompt injection

**Status**: âœ… COMPLETE

#### Tasks
- [x] Write test `test_format_with_content`
- [x] Write test `test_format_fallback_snippet`
- [x] Write test `test_format_mixed_content_and_snippet`
- [x] Write test `test_format_respects_max_chars`
- [x] Update `format_results_for_prompt()` in query_extractor.rs
- [x] Add content prioritization (content > snippet)

**Implementation File**: `src/search/query_extractor.rs` (modify)

```rust
/// Format search results with content for injection into prompt
///
/// Uses actual page content when available, falls back to snippet.
pub fn format_results_with_content(
    results: &[SearchResultWithContent],
    max_total_chars: usize,
) -> String {
    if results.is_empty() {
        return String::new();
    }

    let mut formatted = String::from("[Web Search Results]\n\n");
    let mut total_chars = 0;

    for (i, result) in results.iter().enumerate() {
        if total_chars >= max_total_chars {
            break;
        }

        formatted.push_str(&format!("[{}] {}\n", i + 1, result.title));
        formatted.push_str(&format!("URL: {}\n\n", result.url));

        // Use content if available, otherwise snippet
        let text = if let Some(ref content) = result.content {
            content.clone()
        } else {
            format!("Summary: {}", result.snippet)
        };

        // Truncate if needed to stay within budget
        let remaining = max_total_chars.saturating_sub(total_chars);
        let truncated = if text.len() > remaining {
            format!("{}...", &text[..remaining.saturating_sub(3)])
        } else {
            text
        };

        formatted.push_str(&truncated);
        formatted.push_str("\n\n---\n\n");
        total_chars += truncated.len();
    }

    formatted.push_str("[End Web Search Results]\n");
    formatted
}

#[cfg(test)]
mod tests {
    // ... existing tests ...

    #[test]
    fn test_format_with_content() {
        let results = vec![
            SearchResultWithContent {
                title: "Test Article".to_string(),
                url: "https://example.com".to_string(),
                snippet: "Short snippet".to_string(),
                content: Some("Full article content with lots of detail...".to_string()),
                source: "test".to_string(),
            },
        ];

        let formatted = format_results_with_content(&results, 10000);
        assert!(formatted.contains("Full article content"));
        assert!(!formatted.contains("Short snippet")); // Content takes priority
    }

    #[test]
    fn test_format_fallback_snippet() {
        let results = vec![
            SearchResultWithContent {
                title: "Test Article".to_string(),
                url: "https://example.com".to_string(),
                snippet: "Short snippet description".to_string(),
                content: None, // No content fetched
                source: "test".to_string(),
            },
        ];

        let formatted = format_results_with_content(&results, 10000);
        assert!(formatted.contains("Short snippet")); // Falls back to snippet
    }
}
```

---

### Sub-phase 9.9: Update API Server Integration

**Goal**: Use content fetching in inference handler

**Status**: âœ… COMPLETE

#### Tasks
- [x] Update `handle_inference_request()` to use `search_with_content()`
- [x] Update `handle_streaming_request()` to use `search_with_content()`
- [x] Add logging for content fetch results
- [x] Add metrics for content fetch timing

**Implementation File**: `src/api/server.rs` (modify)

Key change: Replace `search_service.search()` with `search_service.search_with_content()` and use new formatter.

---

### Sub-phase 9.10: Update Version and Documentation

**Goal**: Update version to v8.8.0-content-fetch

**Status**: âœ… COMPLETE

#### Tasks
- [x] Update `/workspace/VERSION` to `8.8.0-content-fetch`
- [x] Update `/workspace/src/version.rs`:
  - [x] VERSION constant
  - [x] VERSION_NUMBER
  - [x] VERSION_MINOR to 8
  - [x] VERSION_PATCH to 0
  - [x] Add feature: "content-fetching"
  - [x] Add feature: "html-extraction"
  - [x] Add to BREAKING_CHANGES
- [x] Update `docs/API.md` with new configuration
- [x] Build: `cargo build --release --features real-ezkl -j 4`
- [x] Test: `cargo test content`

---

### Sub-phase 9.11: Integration Testing

**Goal**: End-to-end testing of content fetching

**Status**: âœ… COMPLETE

#### Tasks
- [x] Run all content tests: `cargo test content`
- [x] Run all search tests: `cargo test search` (105 tests passing)
- [x] Manual test with real URLs:
  ```bash
  curl -X POST http://localhost:8080/v1/inference \
    -H 'Content-Type: application/json' \
    -d '{
      "prompt": "What is the latest news about AI?",
      "web_search": true,
      "stream": false
    }'
  ```
- [ ] Verify logs show content fetch:
  ```
  ğŸ” Web search requested for inference
  ğŸ” Fetching content from: https://bbc.com/news/ai
  ğŸ” Fetched 2847 chars from: https://bbc.com/news/ai
  ğŸ” Web search completed: 3 results with content
  ```
- [ ] Verify LLM response uses actual content (not "I can't browse")
- [ ] Test with WebSocket streaming

---

### Test Summary - Phase 9

| Sub-phase | Test File | Test Count |
|-----------|-----------|------------|
| 9.3 | content/config.rs | 3 |
| 9.4 | content/extractor.rs | 6 |
| 9.5 | content/cache.rs | 4 |
| 9.6 | content/fetcher.rs | 5 |
| 9.7 | service.rs (integration) | 3 |
| 9.8 | query_extractor.rs | 4 |
| **Total** | | **~25 tests** |

---

### Performance Targets - Phase 9

| Operation | Target | Notes |
|-----------|--------|-------|
| Single page fetch | <3s | Including extraction |
| 3 pages parallel | <5s | Total with timeout |
| Cached content | <10ms | Cache hit |
| Content extraction | <50ms | HTML parsing |

---

### Risk Mitigation - Phase 9

| Risk | Level | Mitigation |
|------|-------|------------|
| Slow page loads | Medium | Aggressive timeouts (5s/page, 10s total) |
| Sites blocking bots | Medium | Fall back to snippet gracefully |
| JavaScript-only sites | Low | Accept limitation, use snippet |
| Large pages | Low | Limit fetch size, truncate |
| SSRF attacks | High | Strict URL validation (no localhost/private) |
| Prompt injection | Medium | Sanitize content, escape special chars |

---

### Files Created/Modified - Phase 9

| File | Status | Description |
|------|--------|-------------|
| `Cargo.toml` | Modified | Add `scraper = "0.18"` |
| `src/search/mod.rs` | Modified | Add `pub mod content;` |
| `src/search/content/mod.rs` | **New** | Module exports |
| `src/search/content/config.rs` | **New** | ContentFetchConfig |
| `src/search/content/extractor.rs` | **New** | HTML extraction |
| `src/search/content/cache.rs` | **New** | Content caching |
| `src/search/content/fetcher.rs` | **New** | HTTP fetching |
| `src/search/service.rs` | Modified | Add content_fetcher |
| `src/search/query_extractor.rs` | Modified | Add content formatter |
| `src/api/server.rs` | Modified | Use search_with_content |
| `VERSION` | Modified | Update to 8.8.0 |
| `src/version.rs` | Modified | Update version constants |

---

## Future Enhancements (Out of Scope)

1. **Search result verification** - Hash results for proof submission
2. **Specialized search** - News, images, academic papers
3. **Search history** - Per-session search history for context
4. **Custom search providers** - Allow hosts to add custom providers
5. **Search result ranking** - Re-rank results based on relevance
6. **JavaScript rendering** - Optional headless browser for SPAs (heavy)
7. **Jina Reader fallback** - External service for failed extractions (privacy tradeoff)
