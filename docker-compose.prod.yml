# Fabstir LLM Node Production Docker Compose
# Copyright (c) 2025 Fabstir
# SPDX-License-Identifier: BUSL-1.1
#
# Usage:
#   1. Copy .env.prod.example to .env.prod and configure
#   2. docker-compose -f docker-compose.prod.yml up -d
#
# Required environment variables (in .env.prod):
#   - S5_SEED_PHRASE: S5 identity seed phrase for storage uploads
#   - HOST_PRIVATE_KEY: Host wallet private key for settlements
#

services:
  # S5 Bridge - Must start first for storage uploads
  s5-bridge:
    build:
      context: ./services/s5-bridge
      dockerfile: Dockerfile
    container_name: s5-bridge
    restart: unless-stopped
    env_file:
      - .env
    environment:
      BRIDGE_PORT: 5522
      BRIDGE_HOST: 0.0.0.0
      S5_SEED_PHRASE: ${S5_SEED_PHRASE:?S5_SEED_PHRASE is required}
      S5_PORTAL_URL: ${S5_PORTAL_URL:-https://s5.platformlessai.ai}
      S5_INITIAL_PEERS: ${S5_INITIAL_PEERS:-wss://z2Das8aEF7oNoxkcrfvzerZ1iBPWfm6D7gy3hVE4ALGSpVB@node.sfive.net/s5/p2p,wss://z2DWuWNZcdSyZLpXFK2uCU3haaWMXrDAgxzv17sDEMHstZb@s5.garden/s5/p2p,wss://z2Dh2pH1t1u3mjoQKDrZccLQ1CG9hJe3wdFvLCQhDx5UX1K@s5.vup.cx/s5/p2p}
      HOST_PRIVATE_KEY: ${HOST_PRIVATE_KEY}
      LOG_LEVEL: ${LOG_LEVEL:-info}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5522/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    networks:
      - fabstir-network

  # Fabstir LLM Node - Main inference service
  llm-node:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: llm-node-prod
    restart: unless-stopped
    runtime: nvidia
    env_file:
      - .env
    ports:
      - "${API_PORT:-8080}:${API_PORT:-8080}"      # API port
      - "${P2P_PORT:-9000}:${P2P_PORT:-9000}"      # P2P port
      - "9001:9001"      # P2P port
      - "9002:9002"      # P2P port
    environment:
      # NVIDIA GPU
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility

      # S5 Storage - connects to s5-bridge service
      ENHANCED_S5_URL: http://s5-bridge:5522

      # Model configuration
      MODEL_PATH: ${MODEL_PATH:-/app/models/openai_gpt-oss-20b-MXFP4.gguf}
      LLAMA_BATCH_SIZE: ${LLAMA_BATCH_SIZE:-2048}
      # Thinking/reasoning mode (v8.17.0+): enabled, disabled, low, medium, high
      # When unset, uses template defaults (Harmony: medium, GLM-4: no directive)
      DEFAULT_THINKING_MODE: ${DEFAULT_THINKING_MODE:-}

      # Network configuration
      P2P_PORT: ${P2P_PORT:-9000}
      API_PORT: ${API_PORT:-8080}

      # Blockchain configuration
      HOST_PRIVATE_KEY: ${HOST_PRIVATE_KEY}
      CHAIN_ID: ${CHAIN_ID:-84532}
      RPC_URL: ${RPC_URL}

      # Contract addresses (AUDIT-F4 compliant - Feb 2026)
      CONTRACT_NODE_REGISTRY: ${CONTRACT_NODE_REGISTRY:-0x8BC0Af4aAa2dfb99699B1A24bA85E507de10Fd22}
      CONTRACT_JOB_MARKETPLACE: ${CONTRACT_JOB_MARKETPLACE:-0x95132177F964FF053C1E874b53CF74d819618E06}
      CONTRACT_PROOF_SYSTEM: ${CONTRACT_PROOF_SYSTEM:-0xE8DCa89e1588bbbdc4F7D5F78263632B35401B31}
      CONTRACT_HOST_EARNINGS: ${CONTRACT_HOST_EARNINGS:-0xE4F33e9e132E60fc3477509f99b9E1340b91Aee0}
      CONTRACT_MODEL_REGISTRY: ${CONTRACT_MODEL_REGISTRY:-0x1a9d91521c85bD252Ac848806Ff5096bBb9ACDb2}

      # Model validation (v8.14.0+) - set to true to enforce
      REQUIRE_MODEL_VALIDATION: ${REQUIRE_MODEL_VALIDATION:-false}

      # Logging
      RUST_LOG: ${RUST_LOG:-info}

      # Optional: Web search
      WEB_SEARCH_ENABLED: ${WEB_SEARCH_ENABLED:-true}
      BRAVE_API_KEY: ${BRAVE_API_KEY:-}

      # Optional: VLM Vision sidecar (for high-VRAM hosts)
      VLM_ENDPOINT: ${VLM_ENDPOINT:-http://qwen3-vl:8081}
      VLM_MODEL_NAME: ${VLM_MODEL_NAME:-qwen3-vl}

      # Optional: Diffusion sidecar for image generation (v8.16.0+)
      DIFFUSION_ENDPOINT: ${DIFFUSION_ENDPOINT:-http://diffusion-sidecar:8082}
      DIFFUSION_MODEL_NAME: ${DIFFUSION_MODEL_NAME:-flux2-klein-4b}

      # Auto-route image intent to diffusion sidecar (v8.16.1+)
      # Defaults to true when DIFFUSION_ENDPOINT is set; override with false to disable
      AUTO_IMAGE_ROUTING: ${AUTO_IMAGE_ROUTING:-}

    volumes:
      # Mount model directory
      - ${MODELS_DIR:-./models}:/app/models:ro
      # Mount pre-built binary (from tarball)
      - ./target/release/fabstir-llm-node:/usr/local/bin/fabstir-llm-node:ro
    depends_on:
      s5-bridge:
        condition: service_healthy
    networks:
      - fabstir-network

  # VLM Vision Sidecar (optional - for high-VRAM hosts only)
  # Requires ~5GB VRAM for Qwen3-VL-8B-Instruct Q4_K_M
  # To disable: set VLM_ENDPOINT= (empty) in .env or stop this service
  # First build: docker-compose -f docker-compose.prod.yml build qwen3-vl
  qwen3-vl:
    build:
      context: .
      dockerfile: docker/Dockerfile.vlm-sidecar
    container_name: qwen3-vl
    restart: unless-stopped
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
    volumes:
      - ${VLM_MODEL_PATH:-./models/qwen3-vl}:/models:ro
    command: >
      --model /models/${VLM_MODEL_FILE:-Qwen3VL-8B-Instruct-Q4_K_M.gguf}
      --mmproj /models/${VLM_MMPROJ_FILE:-mmproj-Qwen3VL-8B-Instruct-F16.gguf}
      --host 0.0.0.0 --port 8081
      --ctx-size 4096 --n-gpu-layers 99
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - fabstir-network

  # SGLang Diffusion Sidecar (optional - for image generation)
  # Auto-detects WSL2 and patches NCCL -> gloo backend if needed.
  # --text-encoder-cpu-offload moves the 7.5GB Qwen3 text encoder to CPU RAM.
  # --dit-layerwise-offload true streams transformer blocks one-at-a-time through
  # GPU, keeping only ~0.3-0.5GB on VRAM instead of the full 7.2GB transformer.
  # Combined: sidecar VRAM drops from ~14GB to ~1-2GB (VAE + one block + overhead).
  # PYTORCH_NO_CUDA_MEMORY_CACHING=1 prevents PyTorch from holding freed VRAM,
  # ensuring the LLM can allocate KV cache contexts after image generation.
  # To disable: set DIFFUSION_ENDPOINT= (empty) in .env or stop this service
  # First build: docker-compose -f docker-compose.prod.yml build diffusion-sidecar
  diffusion-sidecar:
    build:
      context: .
      dockerfile: docker/Dockerfile.diffusion-sidecar
    container_name: diffusion-sidecar
    restart: unless-stopped
    runtime: nvidia
    shm_size: '8g'
    ipc: host
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      # Prevent PyTorch from caching freed GPU memory â€” ensures VRAM is returned
      # to the CUDA driver immediately so the LLM can allocate KV cache contexts
      PYTORCH_NO_CUDA_MEMORY_CACHING: "1"
    volumes:
      - ${DIFFUSION_MODEL_PATH:-./models/flux2-klein-4b}:/models/black-forest-labs/FLUX.2-klein-4B
      - ./docker/entrypoint-diffusion.sh:/entrypoint-diffusion.sh:ro
    entrypoint: ["/bin/bash", "/entrypoint-diffusion.sh"]
    command: >
      --model-path /models/black-forest-labs/FLUX.2-klein-4B
      --port 8082
      --host 0.0.0.0
      --text-encoder-cpu-offload
      --dit-layerwise-offload true
      --pin-cpu-memory
      --backend diffusers
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 10s
      start_period: 180s
      retries: 3
    networks:
      - fabstir-network

networks:
  fabstir-network:
    name: fabstir-network
    driver: bridge
