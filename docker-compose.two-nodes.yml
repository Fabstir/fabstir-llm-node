version: '3.8'

services:
  # First LLM Node - WebSocket on port 8080
  llm-node-1:
    image: llm-node-prod:latest
    container_name: llm-node-1
    ports:
      - "9001:9001"  # P2P port
      - "9002:9002"  # P2P port
      - "9003:9003"  # P2P port
      - "8080:8080"  # WebSocket API port
    environment:
      # P2P and API Configuration
      - P2P_PORT=9001
      - API_PORT=8080
      - NODE_ID=node-1
      
      # Model Configuration
      - MODEL_PATH=/models/tiny-vicuna-1b.q4_k_m.gguf
      
      # Proof System Configuration
      - ENABLE_PROOF_GENERATION=true
      - PROOF_TYPE=EZKL
      - PROOF_MODEL_PATH=/models/tiny-vicuna-1b.q4_k_m.gguf
      - PROOF_CACHE_SIZE=100
      - PROOF_BATCH_SIZE=10
      
      # Backend Services
      - VECTOR_DB_URL=http://host.docker.internal:8081
      - S5_URL=http://host.docker.internal:5522
      
      # Logging
      - RUST_LOG=info
    volumes:
      - ./models:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - fabstir-network
    restart: unless-stopped

  # Second LLM Node - WebSocket on port 8081
  llm-node-2:
    image: llm-node-prod:latest
    container_name: llm-node-2
    ports:
      - "9011:9011"  # P2P port (different from node-1)
      - "9012:9012"  # P2P port
      - "9013:9013"  # P2P port
      - "8081:8081"  # WebSocket API port (different from node-1)
    environment:
      # P2P and API Configuration
      - P2P_PORT=9011
      - API_PORT=8081
      - NODE_ID=node-2
      
      # Model Configuration
      - MODEL_PATH=/models/tiny-vicuna-1b.q4_k_m.gguf
      
      # Proof System Configuration
      - ENABLE_PROOF_GENERATION=true
      - PROOF_TYPE=EZKL
      - PROOF_MODEL_PATH=/models/tiny-vicuna-1b.q4_k_m.gguf
      - PROOF_CACHE_SIZE=100
      - PROOF_BATCH_SIZE=10
      
      # Backend Services
      - VECTOR_DB_URL=http://host.docker.internal:8081
      - S5_URL=http://host.docker.internal:5522
      
      # Logging
      - RUST_LOG=info
    volumes:
      - ./models:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - fabstir-network
    restart: unless-stopped

  # Enhanced S5 Storage Service (if not already running)
  s5-server:
    image: fabstir-llm-marketplace-s5-enhanced-dev:latest
    container_name: s5-server
    user: "0:0"
    ports:
      - "5522:5522"
    environment:
      - NODE_ENV=production
      - PORT=5522
      - S5_SEED_PHRASE=${S5_SEED_PHRASE}
    env_file:
      - .env.real_s5
    networks:
      - fabstir-network
    restart: unless-stopped

  # Vector Database Service (if not already running)
  vector-db:
    image: fabstir-vectordb-fabstir-ai-vector-db:latest
    container_name: vector-db
    ports:
      - "8081:8080"
    environment:
      - RUST_LOG=info
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/vectordb
      - S5_MOCK_SERVER_URL=http://s5-server:5522
      - STORAGE_MODE=mock
    networks:
      - fabstir-network
    depends_on:
      - postgres
      - s5-server
    restart: unless-stopped

  # PostgreSQL for Vector DB
  postgres:
    image: postgres:14
    container_name: postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=vectordb
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - fabstir-network
    restart: unless-stopped

networks:
  fabstir-network:
    driver: bridge

volumes:
  postgres-data: